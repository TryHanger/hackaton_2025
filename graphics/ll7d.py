import sqlite3
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.preprocessing import KBinsDiscretizer
from sklearn.neighbors import KernelDensity
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

def compare_with_join():
    """
    –ë–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º JOIN –º–µ–∂–¥—É –ë–î.
    """
    try:
        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –ë–î –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        result_conn = sqlite3.connect(':memory:')
        result_cursor = result_conn.cursor()

        # –ü—Ä–∏—Å–æ–µ–¥–∏–Ω—è–µ–º –æ–±–µ –ë–î
        result_cursor.execute("ATTACH DATABASE '../info_perevody.db' AS info_db")
        result_cursor.execute("ATTACH DATABASE '../perevody.db' AS perevody_db")

        # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        result_cursor.execute("""
            CREATE TABLE logins_last_7_days AS
            SELECT 
                i.cst_dim_id,
                i.transdate,
                i.logins_last_7_days,
                p.target
            FROM info_db.unique_transactions i
            LEFT JOIN perevody_db.unique_transactions p
                ON i.cst_dim_id = p.cst_dim_id 
                AND i.transdate = p.transdate
            WHERE i.logins_last_7_days IS NOT NULL
        """)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        result_cursor.execute("SELECT COUNT(*) FROM logins_last_7_days")
        total_count = result_cursor.fetchone()[0]

        result_cursor.execute("SELECT COUNT(*) FROM logins_last_7_days WHERE target IS NOT NULL")
        matched_count = result_cursor.fetchone()[0]

        result_cursor.execute("SELECT COUNT(*) FROM logins_last_7_days WHERE target = 1")
        target_1_count = result_cursor.fetchone()[0]

        result_cursor.execute("SELECT COUNT(*) FROM logins_last_7_days WHERE target = 0")
        target_0_count = result_cursor.fetchone()[0]

        print("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –°–†–ê–í–ù–ï–ù–ò–Ø:")
        print(f"–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {total_count}")
        print(f"–ù–∞–π–¥–µ–Ω–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π: {matched_count}")
        print(f"–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π: {total_count - matched_count}")
        print(f"Target = 1: {target_1_count}")
        print(f"Target = 0: {target_0_count}")

        # –í—ã–±–æ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        result_cursor.execute("SELECT * FROM logins_last_7_days LIMIT 10")
        sample_results = result_cursor.fetchall()

        print("\nüîç –ü–ï–†–í–´–ï 10 –†–ï–ó–£–õ–¨–¢–ê–¢–û–í:")
        print("ID | –î–∞—Ç–∞ | logins_last_7_days | Target")
        print("-" * 50)
        for row in sample_results:
            print(f"{row[0]} | {row[1]} | {row[2]} | {row[3]}")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–∞–π–ª –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        result_cursor.execute("""
            ATTACH DATABASE '../comparison_results.db' AS results_db
        """)
        result_cursor.execute("""
            CREATE TABLE results_db.logins_last_7_days AS
            SELECT * FROM logins_last_7_days
        """)

        print("\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ comparison_results.db")

        # –û—Ç—Å–æ–µ–¥–∏–Ω—è–µ–º –ë–î
        result_cursor.execute("DETACH DATABASE info_db")
        result_cursor.execute("DETACH DATABASE perevody_db")
        result_cursor.execute("DETACH DATABASE results_db")

        return total_count

    except sqlite3.Error as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö: {e}")
        return 0

def load_data():
    script_dir = Path(__file__).parent
    db_path = script_dir.parent / 'comparison_results.db'
    conn = sqlite3.connect(db_path)
    # conn = sqlite3.connect('../comparison_results.db')
    df = pd.read_sql_query(
        "SELECT logins_last_7_days, target FROM logins_last_7_days WHERE target IS NOT NULL",
        conn
    )
    conn.close()
    return df

def automatic_binning_analysis(df):
    """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –±–∏–Ω–Ω–∏–Ω–≥ —Å —Ä–∞–∑–Ω—ã–º–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏."""

    print("\n" + "=" * 60)
    print("1. –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò–ô –ë–ò–ù–ù–ò–ù–ì")
    print("=" * 60)

    X = df['logins_last_7_days'].values.reshape(-1, 1)

    # –†–∞–∑–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –±–∏–Ω–Ω–∏–Ω–≥–∞
    strategies = {
        'uniform': '–†–∞–≤–Ω–æ–º–µ—Ä–Ω—ã–π',
        'quantile': '–ö–≤–∞–Ω—Ç–∏–ª—å–Ω—ã–π',
        'kmeans': 'K-—Å—Ä–µ–¥–Ω–∏—Ö'
    }

    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    axes = axes.ravel()

    for idx, (strategy, name) in enumerate(strategies.items()):
        # –°–æ–∑–¥–∞–µ–º –±–∏–Ω—ã
        discretizer = KBinsDiscretizer(n_bins=8, encode='ordinal', strategy=strategy)
        bins_array = discretizer.fit_transform(X).flatten()

        # –ê–Ω–∞–ª–∏–∑ fraud rate –ø–æ –±–∏–Ω–∞–º
        df_binned = df.copy()
        df_binned['bin'] = bins_array
        bin_stats = df_binned.groupby('bin').agg({
            'logins_last_7_days': ['min', 'max', 'count'],
            'target': ['sum', 'mean']
        }).round(3)

        bin_stats.columns = ['min_login', 'max_login', 'total_count', 'fraud_count', 'fraud_rate']
        bin_stats['fraud_rate_pct'] = (bin_stats['fraud_rate'] * 100).round(2)

        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
        ax = axes[idx]
        bins_centers = (bin_stats['min_login'] + bin_stats['max_login']) / 2
        ax.plot(bins_centers, bin_stats['fraud_rate_pct'], 'o-', linewidth=2, markersize=8)
        ax.set_title(f'{name} –±–∏–Ω–Ω–∏–Ω–≥\n({strategy})', fontsize=14)
        ax.set_xlabel('–õ–æ–≥–∏–Ω—ã –∑–∞ 7 –¥–Ω–µ–π', fontsize=12)
        ax.set_ylabel('Fraud Rate (%)', fontsize=12)
        ax.grid(True, alpha=0.3)

        # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è
        for i, (center, rate) in enumerate(zip(bins_centers, bin_stats['fraud_rate_pct'])):
            ax.annotate(f'{rate}%', (center, rate), xytext=(0, 10),
                        textcoords='offset points', ha='center', fontsize=9)

        print(f"\n{name} –±–∏–Ω–Ω–∏–Ω–≥:")
        print(bin_stats[['min_login', 'max_login', 'total_count', 'fraud_rate_pct']])

    # –†—É—á–Ω–æ–π –±–∏–Ω–Ω–∏–Ω–≥ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞
    axes[3].axis('off')
    plt.tight_layout()
    plt.show()

    return df

def trend_analysis(df):
    """–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–∞ —Ä–∏—Å–∫–∞."""

    print("\n" + "=" * 60)
    print("2. –°–¢–ê–¢–ò–°–¢–ò–ß–ï–°–ö–ò–ô –ê–ù–ê–õ–ò–ó –¢–†–ï–ù–î–ê")
    print("=" * 60)

    # –°–æ–∑–¥–∞–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –±–∏–Ω—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
    bins = [0, 1, 2, 3, 5, 8, 12, 20, 35, 70, 181]
    labels = ['0', '1', '2', '3-4', '5-7', '8-11', '12-19', '20-34', '35-69', '70+']

    df['bin'] = pd.cut(df['logins_last_7_days'], bins=bins, labels=labels, right=False)
    trend_data = df.groupby('bin').agg({
        'target': ['count', 'sum', 'mean'],
        'logins_last_7_days': 'median'
    }).round(4)

    trend_data.columns = ['total', 'fraud_count', 'fraud_rate', 'median_logins']
    trend_data['fraud_rate_pct'] = (trend_data['fraud_rate'] * 100).round(2)

    # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ—Å—Ç—ã
    X = np.arange(len(trend_data))
    Y = trend_data['fraud_rate'].values

    # –õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è –¥–ª—è —Ç—Ä–µ–Ω–¥–∞
    slope, intercept, r_value, p_value, std_err = stats.linregress(X, Y)

    # –¢–µ—Å—Ç –ú–∞–Ω–Ω–∞-–ö–µ–Ω–¥–∞–ª–ª–∞ (–º–æ–Ω–æ—Ç–æ–Ω–Ω–æ—Å—Ç—å)
    from scipy.stats import kendalltau
    tau, p_tau = kendalltau(X, Y)

    print("üìà –°–¢–ê–¢–ò–°–¢–ò–ö–ê –¢–†–ï–ù–î–ê:")
    print(f"–õ–∏–Ω–µ–π–Ω—ã–π —Ç—Ä–µ–Ω–¥: slope = {slope:.6f}, R¬≤ = {r_value ** 2:.4f}")
    print(f"P-value –ª–∏–Ω–µ–π–Ω–æ—Å—Ç–∏: {p_value:.6f}")
    print(f"–¢–µ—Å—Ç –ö–µ–Ω–¥–∞–ª–ª–∞: tau = {tau:.4f}, p-value = {p_tau:.6f}")
    print(f"–ú–æ–Ω–æ—Ç–æ–Ω–Ω–æ—Å—Ç—å: {'–î–ê' if p_tau < 0.05 and tau > 0 else '–ù–ï–¢'}")

    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ç—Ä–µ–Ω–¥–∞
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

    # –ì—Ä–∞—Ñ–∏–∫ —Ç—Ä–µ–Ω–¥–∞
    ax1.plot(X, Y * 100, 'o-', linewidth=3, markersize=8, label='–ù–∞–±–ª—é–¥–∞–µ–º—ã–π fraud rate')

    # –õ–∏–Ω–µ–π–Ω—ã–π —Ç—Ä–µ–Ω–¥
    trend_line = intercept + slope * X
    ax1.plot(X, trend_line * 100, '--', color='red', linewidth=2,
             label=f'–õ–∏–Ω–µ–π–Ω—ã–π —Ç—Ä–µ–Ω–¥ (R¬≤={r_value ** 2:.3f})')

    ax1.set_title('–ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–∞ Fraud Rate', fontsize=16, fontweight='bold')
    ax1.set_xlabel('–£—Ä–æ–≤–µ–Ω—å –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ (–±–∏–Ω)', fontsize=12)
    ax1.set_ylabel('Fraud Rate (%)', fontsize=12)
    ax1.set_xticks(X)
    ax1.set_xticklabels(labels, rotation=45)
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è
    for i, (x, y) in enumerate(zip(X, Y * 100)):
        ax1.annotate(f'{y:.1f}%', (x, y), xytext=(0, 10),
                     textcoords='offset points', ha='center', fontweight='bold')

    # –î–µ—Ç–∞–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞
    ax2.axis('off')
    table_data = []
    for bin_name in labels:
        if bin_name in trend_data.index:
            row = trend_data.loc[bin_name]
            table_data.append([
                bin_name, row['total'], row['fraud_count'],
                f"{row['fraud_rate_pct']}%", f"{row['median_logins']:.1f}"
            ])

    table = ax2.table(cellText=table_data,
                      colLabels=['–ë–∏–Ω', '–í—Å–µ–≥–æ', 'Fraud', 'Fraud%', '–ú–µ–¥–∏–∞–Ω–∞'],
                      loc='center',
                      cellLoc='center')
    table.auto_set_font_size(False)
    table.set_fontsize(10)
    table.scale(1, 1.5)
    ax2.set_title('–î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –±–∏–Ω–∞–º', fontsize=14, fontweight='bold')

    plt.tight_layout()
    plt.show()

    return trend_data

def tail_analysis(df):
    """–ê–Ω–∞–ª–∏–∑ —Ä–µ–¥–∫–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π (—Ö–≤–æ—Å—Ç–æ–≤ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è)."""

    print("\n" + "=" * 60)
    print("3. –ê–ù–ê–õ–ò–ó –•–í–û–°–¢–û–í –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–Ø")
    print("=" * 60)

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤—ã–±—Ä–æ—Å—ã (–±–æ–ª–µ–µ 3 —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–π)
    mean_logins = df['logins_last_7_days'].mean()
    std_logins = df['logins_last_7_days'].std()
    outlier_threshold = mean_logins + 3 * std_logins

    outliers = df[df['logins_last_7_days'] > outlier_threshold]
    normal_data = df[df['logins_last_7_days'] <= outlier_threshold]

    print(f"–ü–æ—Ä–æ–≥ –≤—ã–±—Ä–æ—Å–æ–≤: > {outlier_threshold:.1f} –ª–æ–≥–∏–Ω–æ–≤")
    print(f"–í—ã–±—Ä–æ—Å–æ–≤: {len(outliers)} –∑–∞–ø–∏—Å–µ–π ({len(outliers) / len(df) * 100:.2f}%)")
    print(f"–í—ã–±—Ä–æ—Å—ã - Fraud Rate: {outliers['target'].mean() * 100:.2f}%")
    print(f"–ù–æ—Ä–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è - Fraud Rate: {normal_data['target'].mean() * 100:.2f}%")

    # –ê–Ω–∞–ª–∏–∑ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    extreme_thresholds = [50, 100, 150]

    print("\nüîç –ê–ù–ê–õ–ò–ó –≠–ö–°–¢–†–ï–ú–ê–õ–¨–ù–´–• –ó–ù–ê–ß–ï–ù–ò–ô:")
    for threshold in extreme_thresholds:
        extreme_data = df[df['logins_last_7_days'] >= threshold]
        if len(extreme_data) > 0:
            fraud_rate = extreme_data['target'].mean() * 100
            print(f"‚â•{threshold} –ª–æ–≥–∏–Ω–æ–≤: {len(extreme_data)} –∑–∞–ø–∏—Å–µ–π, Fraud Rate: {fraud_rate:.2f}%")

    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ö–≤–æ—Å—Ç–æ–≤
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å –≤—ã–¥–µ–ª–µ–Ω–∏–µ–º –≤—ã–±—Ä–æ—Å–æ–≤
    ax1.hist(normal_data['logins_last_7_days'], bins=50, alpha=0.7,
             color='blue', label=f'–ù–æ—Ä–º–∞–ª—å–Ω—ã–µ (‚â§{outlier_threshold:.0f})')
    ax1.hist(outliers['logins_last_7_days'], bins=20, alpha=0.7,
             color='red', label=f'–í—ã–±—Ä–æ—Å—ã (> {outlier_threshold:.0f})')
    ax1.axvline(outlier_threshold, color='black', linestyle='--', linewidth=2, label='–ü–æ—Ä–æ–≥ –≤—ã–±—Ä–æ—Å–æ–≤')
    ax1.set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å –≤—ã–¥–µ–ª–µ–Ω–∏–µ–º –≤—ã–±—Ä–æ—Å–æ–≤', fontsize=14)
    ax1.set_xlabel('–õ–æ–≥–∏–Ω—ã –∑–∞ 7 –¥–Ω–µ–π', fontsize=12)
    ax1.set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞', fontsize=12)
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # Fraud rate –ø–æ –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—è–º - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –ß–ê–°–¢–¨
    percentiles = np.arange(0, 101, 5)
    percentile_values = np.percentile(df['logins_last_7_days'], percentiles)
    percentile_fraud = []

    for p in percentiles:
        threshold = np.percentile(df['logins_last_7_days'], p)
        high_activity = df[df['logins_last_7_days'] >= threshold]
        if len(high_activity) > 0:
            fraud_rate = high_activity['target'].mean() * 100
            percentile_fraud.append(fraud_rate)
        else:
            percentile_fraud.append(0)

    ax2.plot(100 - percentiles, percentile_fraud, 'o-', linewidth=2, markersize=4)
    ax2.set_title('Fraud Rate –ø–æ –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—è–º –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏\n(—Ç–æ–ø X% —Å–∞–º—ã—Ö –∞–∫—Ç–∏–≤–Ω—ã—Ö)', fontsize=14)
    ax2.set_xlabel('–¢–æ–ø X% —Å–∞–º—ã—Ö –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π', fontsize=12)
    ax2.set_ylabel('Fraud Rate (%)', fontsize=12)
    ax2.grid(True, alpha=0.3)
    ax2.invert_xaxis()

    # –î–æ–±–∞–≤–ª—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Ç–æ—á–∫–∏ - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –ß–ê–°–¢–¨
    key_percentiles = [95, 99, 99.9]
    for p in key_percentiles:
        # –ù–∞—Ö–æ–¥–∏–º –±–ª–∏–∂–∞–π—à–∏–π –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å –∏–∑ –Ω–∞—à–µ–≥–æ —Å–ø–∏—Å–∫–∞
        closest_p = min(percentiles, key=lambda x: abs(x - p))
        idx = list(percentiles).index(closest_p)

        # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—è
        actual_threshold = np.percentile(df['logins_last_7_days'], p)

        ax2.annotate(f'{p}%: {percentile_fraud[idx]:.1f}%\n(‚â•{actual_threshold:.0f} –ª–æ–≥–∏–Ω–æ–≤)',
                     (100 - closest_p, percentile_fraud[idx]),
                     xytext=(10, 5), textcoords='offset points',
                     bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7),
                     fontsize=9)

    plt.tight_layout()
    plt.show()

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–∞–º—ã—Ö –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
    print("\nüìä –ê–ù–ê–õ–ò–ó –°–ê–ú–´–• –ê–ö–¢–ò–í–ù–´–• –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–ï–ô:")
    for p in [99, 99.5, 99.9]:
        threshold = np.percentile(df['logins_last_7_days'], p)
        top_users = df[df['logins_last_7_days'] >= threshold]
        if len(top_users) > 0:
            fraud_rate = top_users['target'].mean() * 100
            print(
                f"–¢–æ–ø {100 - p:.1f}% (‚â•{threshold:.0f} –ª–æ–≥–∏–Ω–æ–≤): {len(top_users)} –∑–∞–ø–∏—Å–µ–π, Fraud Rate: {fraud_rate:.2f}%")

    return outliers

def nonlinear_analysis(df):
    """–ü–æ–∏—Å–∫ –Ω–µ–ª–∏–Ω–µ–π–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π."""

    print("\n" + "=" * 60)
    print("4. –ü–û–ò–°–ö –ù–ï–õ–ò–ù–ï–ô–ù–´–• –ó–ê–í–ò–°–ò–ú–û–°–¢–ï–ô")
    print("=" * 60)

    # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –¥–∞–Ω–Ω—ã—Ö –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º
    sorted_df = df.copy().sort_values('logins_last_7_days').reset_index(drop=True)

    # –°–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ –¥–ª—è —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è (—É–±–∏—Ä–∞–µ–º NaN –≤ –∫–æ–Ω—Ü–µ)
    sorted_df['rolling_fraud'] = sorted_df['target'].rolling(window=100, center=True).mean()

    # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NaN –≤ rolling_fraud
    valid_data = sorted_df.dropna(subset=['rolling_fraud']).copy()

    # –ü–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω–∞—è –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—è
    X_poly = valid_data['logins_last_7_days'].values
    Y_poly = valid_data['rolling_fraud'].values

    # –ê–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏–∏ —Ä–∞–∑–Ω–æ–π —Å—Ç–µ–ø–µ–Ω–∏
    degrees = [1, 2, 3, 4]
    colors = ['red', 'blue', 'green', 'purple']

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

    # –ì—Ä–∞—Ñ–∏–∫ 1: –°–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ
    ax1.scatter(sorted_df['logins_last_7_days'], sorted_df['target'],
                alpha=0.1, color='gray', s=1, label='–ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ')
    ax1.plot(valid_data['logins_last_7_days'], valid_data['rolling_fraud'] * 100,
             linewidth=3, color='black', label='–°–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ (n=100)')
    ax1.set_title('Fraud Rate vs –õ–æ–≥–∏–Ω—ã\n(—Å–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ)', fontsize=14)
    ax1.set_xlabel('–õ–æ–≥–∏–Ω—ã –∑–∞ 7 –¥–Ω–µ–π', fontsize=12)
    ax1.set_ylabel('Fraud Rate (%)', fontsize=12)
    ax1.set_xlim(0, 100)
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # –ì—Ä–∞—Ñ–∏–∫ 2: –ü–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω–∞—è –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—è
    ax2.scatter(X_poly, Y_poly * 100, alpha=0.3, color='gray', s=10, label='–°–≥–ª–∞–∂–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ')

    r_squared = {}
    for degree, color in zip(degrees, colors):
        try:
            coeffs = np.polyfit(X_poly, Y_poly, degree)
            polynomial = np.poly1d(coeffs)
            y_fit = polynomial(X_poly)

            # R¬≤
            ss_res = np.sum((Y_poly - y_fit) ** 2)
            ss_tot = np.sum((Y_poly - np.mean(Y_poly)) ** 2)
            r_sq = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0
            r_squared[degree] = r_sq

            ax2.plot(X_poly, y_fit * 100, color=color, linewidth=2,
                     label=f'–ü–æ–ª–∏–Ω–æ–º {degree} —Å—Ç–µ–ø–µ–Ω–∏ (R¬≤={r_sq:.3f})')
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏–∏ —Å—Ç–µ–ø–µ–Ω–∏ {degree}: {e}")
            r_squared[degree] = 0

    ax2.set_title('–ü–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω–∞—è –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—è', fontsize=14)
    ax2.set_xlabel('–õ–æ–≥–∏–Ω—ã –∑–∞ 7 –¥–Ω–µ–π', fontsize=12)
    ax2.set_ylabel('Fraud Rate (%)', fontsize=12)
    ax2.set_xlim(0, 100)
    ax2.legend()
    ax2.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

    # –ê–Ω–∞–ª–∏–∑ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —Å—Ç–µ–ø–µ–Ω–∏
    print("üìä –ö–ê–ß–ï–°–¢–í–û –ê–ü–ü–†–û–ö–°–ò–ú–ê–¶–ò–ò:")
    for degree, r_sq in r_squared.items():
        print(f"–ü–æ–ª–∏–Ω–æ–º {degree} —Å—Ç–µ–ø–µ–Ω–∏: R¬≤ = {r_sq:.4f}")

    # –õ–æ–∫–∞–ª—å–Ω—ã–µ –º–∞–∫—Å–∏–º—É–º—ã/–º–∏–Ω–∏–º—É–º—ã –¥–ª—è –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
    if r_squared:
        optimal_degree = max(r_squared, key=r_squared.get)
        try:
            coeffs = np.polyfit(X_poly, Y_poly, optimal_degree)
            polynomial = np.poly1d(coeffs)

            # –ü—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è –¥–ª—è –ø–æ–∏—Å–∫–∞ —ç–∫—Å—Ç—Ä–µ–º—É–º–æ–≤
            derivative = polynomial.deriv()
            critical_points = derivative.roots

            # –§–∏–ª—å—Ç—Ä—É–µ–º –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ —Ç–æ—á–∫–∏ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ –¥–∞–Ω–Ω—ã—Ö
            real_critical_points = critical_points[np.isreal(critical_points)].real
            real_critical_points = real_critical_points[
                (real_critical_points >= X_poly.min()) &
                (real_critical_points <= X_poly.max())
                ]

            print(f"\nüîç –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –¢–û–ß–ö–ò (–ø–æ–ª–∏–Ω–æ–º {optimal_degree} —Å—Ç–µ–ø–µ–Ω–∏):")
            if len(real_critical_points) > 0:
                for point in real_critical_points:
                    fraud_at_point = polynomial(point) * 100
                    point_type = "–ú–ê–ö–°–ò–ú–£–ú" if polynomial.deriv(2)(point) < 0 else "–º–∏–Ω–∏–º—É–º"
                    print(f"  {point:.1f} –ª–æ–≥–∏–Ω–æ–≤: Fraud Rate = {fraud_at_point:.2f}% ({point_type})")
            else:
                print("  –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ —Ç–æ—á–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–æ—á–µ–∫: {e}")

    return r_squared

def kde_analysis(df):
    """Kernel Density Estimation –¥–ª—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π."""

    print("\n" + "=" * 60)
    print("5. KERNEL DENSITY ESTIMATION")
    print("=" * 60)

    logins_0 = df[df['target'] == 0]['logins_last_7_days']
    logins_1 = df[df['target'] == 1]['logins_last_7_days']

    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω –¥–ª—è –ª—É—á—à–µ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
    max_val = 50
    logins_0_trimmed = logins_0[logins_0 <= max_val]
    logins_1_trimmed = logins_1[logins_1 <= max_val]

    # KDE —Å —Ä–∞–∑–Ω—ã–º–∏ bandwidth
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))

    bandwidths = [0.5, 1.0, 2.0, 'auto']
    titles = ['Small bandwidth (0.5)', 'Medium bandwidth (1.0)',
              'Large bandwidth (2.0)', 'Auto bandwidth']

    for idx, (bw, title) in enumerate(zip(bandwidths, titles)):
        ax = axes[idx // 2, idx % 2]

        # KDE –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
        if bw == 'auto':
            kde_0 = stats.gaussian_kde(logins_0_trimmed)
            kde_1 = stats.gaussian_kde(logins_1_trimmed)
        else:
            kde_0 = stats.gaussian_kde(logins_0_trimmed, bw_method=bw)
            kde_1 = stats.gaussian_kde(logins_1_trimmed, bw_method=bw)

        x_range = np.linspace(0, max_val, 200)

        ax.plot(x_range, kde_0(x_range), label='Target=0 (–õ–µ–≥–∏—Ç–∏–º–Ω—ã–µ)',
                color='blue', linewidth=2)
        ax.plot(x_range, kde_1(x_range), label='Target=1 (–ú–æ—à–µ–Ω–Ω–∏—á–µ—Å–∫–∏–µ)',
                color='red', linewidth=2)

        ax.set_title(f'KDE: {title}', fontsize=14)
        ax.set_xlabel('–õ–æ–≥–∏–Ω—ã –∑–∞ 7 –¥–Ω–µ–π', fontsize=12)
        ax.set_ylabel('–ü–ª–æ—Ç–Ω–æ—Å—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏', fontsize=12)
        ax.legend()
        ax.grid(True, alpha=0.3)
        ax.set_xlim(0, max_val)

    plt.tight_layout()
    plt.show()

    # Ratio of densities (—Å–∏–≥–Ω–∞–ª –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏)
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

    # Auto KDE –¥–ª—è ratio –∞–Ω–∞–ª–∏–∑–∞
    kde_0_auto = stats.gaussian_kde(logins_0_trimmed)
    kde_1_auto = stats.gaussian_kde(logins_1_trimmed)

    x_range = np.linspace(0, max_val, 200)
    density_0 = kde_0_auto(x_range)
    density_1 = kde_1_auto(x_range)

    # –û—Ç–Ω–æ—à–µ–Ω–∏–µ –ø–ª–æ—Ç–Ω–æ—Å—Ç–µ–π
    likelihood_ratio = np.where(density_0 > 0, density_1 / density_0, 0)

    ax1.plot(x_range, density_0, label='P(x|Target=0)', color='blue', linewidth=2)
    ax1.plot(x_range, density_1, label='P(x|Target=1)', color='red', linewidth=2)
    ax1.set_title('–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è', fontsize=14)
    ax1.set_xlabel('–õ–æ–≥–∏–Ω—ã –∑–∞ 7 –¥–Ω–µ–π', fontsize=12)
    ax1.set_ylabel('–ü–ª–æ—Ç–Ω–æ—Å—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏', fontsize=12)
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    ax2.plot(x_range, likelihood_ratio, color='purple', linewidth=3)
    ax2.axhline(y=1, color='black', linestyle='--', alpha=0.5, label='–ü–æ—Ä–æ–≥ (ratio=1)')
    ax2.set_title('Likelihood Ratio: P(x|Target=1) / P(x|Target=0)', fontsize=14)
    ax2.set_xlabel('–õ–æ–≥–∏–Ω—ã –∑–∞ 7 –¥–Ω–µ–π', fontsize=12)
    ax2.set_ylabel('Likelihood Ratio', fontsize=12)
    ax2.legend()
    ax2.grid(True, alpha=0.3)

    # –û–±–ª–∞—Å—Ç–∏ –≥–¥–µ –º–æ—à–µ–Ω–Ω–∏–∫–∏ –±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—ã
    fraud_preferred = x_range[likelihood_ratio > 1]
    if len(fraud_preferred) > 0:
        print(f"üîç –û–ë–õ–ê–°–¢–ò –ü–†–ï–ò–ú–£–©–ï–°–¢–í–ê –ú–û–®–ï–ù–ù–ò–ö–û–í:")
        print(f"  –õ–æ–≥–∏–Ω—ã –æ—Ç {fraud_preferred[0]:.1f} –¥–æ {fraud_preferred[-1]:.1f}")

        # –ü–∏–∫ –æ—Ç–Ω–æ—à–µ–Ω–∏—è –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏—è
        peak_idx = np.argmax(likelihood_ratio)
        peak_x = x_range[peak_idx]
        peak_ratio = likelihood_ratio[peak_idx]
        print(f"  –ü–∏–∫ –æ—Ç–Ω–æ—à–µ–Ω–∏—è: {peak_ratio:.2f} –ø—Ä–∏ {peak_x:.1f} –ª–æ–≥–∏–Ω–∞—Ö")

    plt.tight_layout()
    plt.show()

    return likelihood_ratio, x_range

def final_report(df, trend_data, r_squared, likelihood_ratio, x_range):
    """–§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç —Å –≤—ã–≤–æ–¥–∞–º–∏."""

    print("\n" + "=" * 80)
    print("üéØ –ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢ –ê–ù–ê–õ–ò–ó–ê")
    print("=" * 80)

    # –ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    total_fraud_rate = df['target'].mean() * 100
    correlation = df['logins_last_7_days'].corr(df['target'])

    print("üìà –ö–õ–Æ–ß–ï–í–´–ï –ú–ï–¢–†–ò–ö–ò:")
    print(f"  ‚Ä¢ –û–±—â–∏–π Fraud Rate: {total_fraud_rate:.2f}%")
    print(f"  ‚Ä¢ –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ª–æ–≥–∏–Ω—ã-—Ä–∏—Å–∫: {correlation:.4f}")
    print(f"  ‚Ä¢ –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è —Å—Ç–µ–ø–µ–Ω—å –ø–æ–ª–∏–Ω–æ–º–∞: {max(r_squared, key=r_squared.get)}")
    print(f"  ‚Ä¢ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π likelihood ratio: {np.max(likelihood_ratio):.2f}")

    # –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    print("\nüí° –ü–†–ê–ö–¢–ò–ß–ï–°–ö–ò–ï –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")

    # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–æ—Ä–æ–≥–∏
    high_risk_threshold = 20
    medium_risk_threshold = 5

    high_risk_data = df[df['logins_last_7_days'] >= high_risk_threshold]
    medium_risk_data = df[(df['logins_last_7_days'] >= medium_risk_threshold) &
                          (df['logins_last_7_days'] < high_risk_threshold)]

    print(f"  1. –í–´–°–û–ö–ò–ô –†–ò–°–ö (‚â•{high_risk_threshold} –ª–æ–≥–∏–Ω–æ–≤):")
    print(f"     ‚Ä¢ {len(high_risk_data)} –∑–∞–ø–∏—Å–µ–π ({len(high_risk_data) / len(df) * 100:.1f}%)")
    print(f"     ‚Ä¢ Fraud Rate: {high_risk_data['target'].mean() * 100:.2f}%")

    print(f"  2. –°–†–ï–î–ù–ò–ô –†–ò–°–ö ({medium_risk_threshold}-{high_risk_threshold - 1} –ª–æ–≥–∏–Ω–æ–≤):")
    print(f"     ‚Ä¢ {len(medium_risk_data)} –∑–∞–ø–∏—Å–µ–π ({len(medium_risk_data) / len(df) * 100:.1f}%)")
    print(f"     ‚Ä¢ Fraud Rate: {medium_risk_data['target'].mean() * 100:.2f}%")

    # –û–±–ª–∞—Å—Ç–∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —Å–∏–≥–Ω–∞–ª–∞
    peak_signal_idx = np.argmax(likelihood_ratio)
    peak_signal_x = x_range[peak_signal_idx]

    print(f"  3. –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–´–ô –°–ò–ì–ù–ê–õ:")
    print(f"     ‚Ä¢ {peak_signal_x:.1f} –ª–æ–≥–∏–Ω–æ–≤ (likelihood ratio = {np.max(likelihood_ratio):.2f})")

    # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –∑–Ω–∞—á–∏–º–æ—Å—Ç—å
    from scipy.stats import mannwhitneyu
    stat, p_value = mannwhitneyu(logins_0, logins_1, alternative='two-sided')

    print(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ß–ï–°–ö–ê–Ø –ó–ù–ê–ß–ò–ú–û–°–¢–¨:")
    print(f"  ‚Ä¢ –¢–µ—Å—Ç –ú–∞–Ω–Ω–∞-–£–∏—Ç–Ω–∏: p-value = {p_value:.6f}")
    print(f"  ‚Ä¢ –†–∞–∑–ª–∏—á–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π: {'–ó–ù–ê–ß–ò–ú–û' if p_value < 0.05 else '–ù–ï–ó–ù–ê–ß–ò–ú–û'}")

    # –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    plt.figure(figsize=(12, 8))

    # –ö–æ–º–ø–æ–∑–∏—Ç–Ω—ã–π –≥—Ä–∞—Ñ–∏–∫
    plt.subplot(2, 1, 1)

    # KDE —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
    kde_0 = stats.gaussian_kde(logins_0[logins_0 <= 50])
    kde_1 = stats.gaussian_kde(logins_1[logins_1 <= 50])
    x_kde = np.linspace(0, 50, 200)

    plt.plot(x_kde, kde_0(x_kde), label='–õ–µ–≥–∏—Ç–∏–º–Ω—ã–µ (Target=0)', color='blue', linewidth=2)
    plt.plot(x_kde, kde_1(x_kde), label='–ú–æ—à–µ–Ω–Ω–∏—á–µ—Å–∫–∏–µ (Target=1)', color='red', linewidth=2)
    plt.title('–ò—Ç–æ–≥–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑: –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏ Risk Trend', fontsize=16, fontweight='bold')
    plt.xlabel('–õ–æ–≥–∏–Ω—ã –∑–∞ 7 –¥–Ω–µ–π', fontsize=12)
    plt.ylabel('–ü–ª–æ—Ç–Ω–æ—Å—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏', fontsize=12)
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.subplot(2, 1, 2)

    # Fraud rate —Ç—Ä–µ–Ω–¥
    plt.plot(trend_data.index, trend_data['fraud_rate_pct'], 'o-',
             linewidth=3, markersize=8, color='green', label='–ù–∞–±–ª—é–¥–∞–µ–º—ã–π Fraud Rate')
    plt.axhline(y=total_fraud_rate, color='black', linestyle='--',
                label=f'–û–±—â–∏–π Fraud Rate ({total_fraud_rate:.2f}%)')
    plt.title('Fraud Rate –ø–æ —É—Ä–æ–≤–Ω—è–º –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏', fontsize=14)
    plt.xlabel('–£—Ä–æ–≤–µ–Ω—å –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏', fontsize=12)
    plt.ylabel('Fraud Rate (%)', fontsize=12)
    plt.xticks(rotation=45)
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

    print("\n‚úÖ –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù!")

def calculate_detailed_stats(df, value_column='logins_last_7_days', target_column='target'):
    """
    –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –≥—Ä—É–ø–ø–∞–º target, –≤–∫–ª—é—á–∞—è RRN –∏ SE.
    –ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–æ –ø–æ–¥ –¥–∞–Ω–Ω—ã–µ login_frequency_7d.

    Parameters:
    -----------
    df : pandas.DataFrame
        DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏
    value_column : str
        –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Å —á–∏—Å–ª–æ–≤—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    target_column : str
        –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Å —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π

    Returns:
    --------
    pandas.DataFrame
        DataFrame —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞–º–∏ –ø–æ –≥—Ä—É–ø–ø–∞–º
    """

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∫–æ–ª–æ–Ω–∫–∏ —Å—É—â–µ—Å—Ç–≤—É—é—Ç
    if value_column not in df.columns:
        available_cols = df.columns.tolist()
        raise ValueError(f"–ö–æ–ª–æ–Ω–∫–∞ {value_column} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ DataFrame. –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {available_cols}")
    if target_column not in df.columns:
        available_cols = df.columns.tolist()
        raise ValueError(f"–ö–æ–ª–æ–Ω–∫–∞ {target_column} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ DataFrame. –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {available_cols}")

    # –£–±–∏—Ä–∞–µ–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
    df_clean = df[[value_column, target_column]].dropna()

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º target –≤ —Ü–µ–ª—ã–µ —á–∏—Å–ª–∞, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    df_clean[target_column] = df_clean[target_column].astype(int)

    print(f"üìä –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö:")
    print(f"   –ö–æ–ª–æ–Ω–∫–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {value_column}")
    print(f"   –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(df_clean)}")
    print(f"   –î–∏–∞–ø–∞–∑–æ–Ω –∑–Ω–∞—á–µ–Ω–∏–π: {df_clean[value_column].min():.4f} - {df_clean[value_column].max():.4f}")

    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ target
    grouped = df_clean.groupby(target_column)

    stats_list = []

    for target_val, group in grouped:
        values = group[value_column]
        n = len(values)

        print(f"   Target {target_val}: {n} –∑–∞–ø–∏—Å–µ–π")

        # –û—Å–Ω–æ–≤–Ω—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        mean_val = values.mean()
        median_val = values.median()
        std_val = values.std()
        min_val = values.min()
        max_val = values.max()
        q25 = values.quantile(0.25)
        q75 = values.quantile(0.75)

        # –ü—Ä–æ—Ü–µ–Ω—Ç –Ω–µ–Ω—É–ª–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        pct_nonzero = (values != 0).sum() / n * 100 if n > 0 else 0

        # RRN (Robust Relative Norm) - —É—Å—Ç–æ–π—á–∏–≤–∞—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞
        if median_val != 0:
            mad = np.median(np.abs(values - median_val))  # Median Absolute Deviation
            rrn = mad / median_val
        else:
            # –ï—Å–ª–∏ –º–µ–¥–∏–∞–Ω–∞ —Ä–∞–≤–Ω–∞ 0, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ä–µ–¥–Ω–µ–µ –∞–±—Å–æ–ª—é—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –æ—Ç 0
            rrn = np.mean(np.abs(values)) if len(values) > 0 else 0

        # SE (Standard Error) - —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ —Å—Ä–µ–¥–Ω–µ–≥–æ
        se = std_val / np.sqrt(n) if n > 0 else 0

        # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –≤–∞—Ä–∏–∞—Ü–∏–∏
        cv = (std_val / mean_val * 100) if mean_val != 0 else 0

        # –ê—Å–∏–º–º–µ—Ç—Ä–∏—è –∏ —ç–∫—Å—Ü–µ—Å—Å
        skewness = stats.skew(values) if len(values) > 2 else 0
        kurtosis = stats.kurtosis(values) if len(values) > 2 else 0

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —á–∞—Å—Ç–æ—Ç—ã –ª–æ–≥–∏–Ω–æ–≤
        # –ü—Ä–æ—Ü–µ–Ω—Ç –æ—á–µ–Ω—å –Ω–∏–∑–∫–æ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ (<0.1)
        pct_very_low = (values < 0.1).sum() / n * 100
        # –ü—Ä–æ—Ü–µ–Ω—Ç –≤—ã—Å–æ–∫–æ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ (>5)
        pct_high = (values > 5).sum() / n * 100
        # –ü—Ä–æ—á–µ–Ω—å –æ—á–µ–Ω—å –≤—ã—Å–æ–∫–æ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ (>10)
        pct_very_high = (values > 10).sum() / n * 100

        stats_list.append({
            'target': target_val,
            'n': n,
            'mean': mean_val,
            'median': median_val,
            'std': std_val,
            'min': min_val,
            'max': max_val,
            'q25': q25,
            'q75': q75,
            'pct_nonzero': pct_nonzero,
            'rrn': rrn,
            'se': se,
            'cv': cv,
            'skewness': skewness,
            'kurtosis': kurtosis,
            'pct_very_low': pct_very_low,
            'pct_high': pct_high,
            'pct_very_high': pct_very_high
        })

    # –°–æ–∑–¥–∞–µ–º DataFrame —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞–º–∏
    stats_df = pd.DataFrame(stats_list)

    # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É (–≤—Å–µ –¥–∞–Ω–Ω—ã–µ)
    all_values = df_clean[value_column]
    n_total = len(all_values)
    mean_total = all_values.mean()
    median_total = all_values.median()
    std_total = all_values.std()

    if median_total != 0:
        mad_total = np.median(np.abs(all_values - median_total))
        rrn_total = mad_total / median_total
    else:
        rrn_total = np.mean(np.abs(all_values)) if len(all_values) > 0 else 0

    se_total = std_total / np.sqrt(n_total) if n_total > 0 else 0

    total_stats = {
        'target': 'all',
        'n': n_total,
        'mean': mean_total,
        'median': median_total,
        'std': std_total,
        'min': all_values.min(),
        'max': all_values.max(),
        'q25': all_values.quantile(0.25),
        'q75': all_values.quantile(0.75),
        'pct_nonzero': (all_values != 0).sum() / n_total * 100,
        'rrn': rrn_total,
        'se': se_total,
        'cv': (std_total / mean_total * 100) if mean_total != 0 else 0,
        'skewness': stats.skew(all_values) if len(all_values) > 2 else 0,
        'kurtosis': stats.kurtosis(all_values) if len(all_values) > 2 else 0,
        'pct_very_low': (all_values < 0.1).sum() / n_total * 100,
        'pct_high': (all_values > 5).sum() / n_total * 100,
        'pct_very_high': (all_values > 10).sum() / n_total * 100
    }

    stats_df = pd.concat([stats_df, pd.DataFrame([total_stats])], ignore_index=True)

    return stats_df

def print_detailed_stats(stats_df):
    """–ö—Ä–∞—Å–∏–≤–æ –≤—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–ª—è login_frequency_7d."""

    print("\n" + "=" * 100)
    print("üìä –î–ï–¢–ê–õ–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –î–õ–Ø logins_last_7_days")
    print("=" * 100)

    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –≤—ã–≤–æ–¥
    formatted_df = stats_df.copy()

    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    float_format_1 = ['mean', 'median', 'std', 'min', 'max', 'q25', 'q75']
    float_format_2 = ['pct_nonzero', 'cv', 'pct_very_low', 'pct_high', 'pct_very_high']
    float_format_3 = ['rrn', 'se', 'skewness', 'kurtosis']

    for col in float_format_1:
        if col in formatted_df.columns:
            formatted_df[col] = formatted_df[col].apply(lambda x: f"{x:.4f}" if pd.notnull(x) else "NaN")

    for col in float_format_2:
        if col in formatted_df.columns:
            formatted_df[col] = formatted_df[col].apply(lambda x: f"{x:.2f}%" if pd.notnull(x) else "NaN")

    for col in float_format_3:
        if col in formatted_df.columns:
            formatted_df[col] = formatted_df[col].apply(lambda x: f"{x:.6f}" if pd.notnull(x) else "NaN")

    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º n –∫–∞–∫ —Ü–µ–ª–æ–µ —á–∏—Å–ª–æ
    formatted_df['n'] = formatted_df['n'].apply(lambda x: f"{int(x)}" if pd.notnull(x) else "NaN")

    # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∫—Ä–∞—Å–∏–≤–æ–≥–æ –≤—ã–≤–æ–¥–∞
    display_columns = {
        'target': 'Target',
        'n': 'N',
        'mean': 'Mean',
        'median': 'Median',
        'std': 'Std',
        'min': 'Min',
        'max': 'Max',
        'q25': 'Q25',
        'q75': 'Q75',
        'pct_nonzero': 'NonZero%',
        'rrn': 'RRN',
        'se': 'SE',
        'cv': 'CV%',
        'skewness': 'Skewness',
        'kurtosis': 'Kurtosis',
        'pct_very_low': '<0.1%',
        'pct_high': '>5%',
        'pct_very_high': '>10%'
    }

    # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ —Ç–æ–ª—å–∫–æ –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –∫–æ–ª–æ–Ω–∫–∞–º
    existing_columns = {k: v for k, v in display_columns.items() if k in formatted_df.columns}
    formatted_df = formatted_df.rename(columns=existing_columns)

    # –í—ã–≤–æ–¥–∏–º –æ—Å–Ω–æ–≤–Ω—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    print("\nüéØ –û–°–ù–û–í–ù–´–ï –°–¢–ê–¢–ò–°–¢–ò–ö–ò:")
    basic_columns = ['Target', 'N', 'Mean', 'Median', 'Std', 'Min', 'Max', 'Q25', 'Q75', 'NonZero%']
    basic_columns = [col for col in basic_columns if col in formatted_df.columns]
    print(formatted_df[basic_columns].to_string(index=False))

    print("\nüìà –ú–ï–¢–†–ò–ö–ò –ò–ó–ú–ï–ù–ß–ò–í–û–°–¢–ò:")
    variability_columns = ['Target', 'RRN', 'SE', 'CV%', 'Skewness', 'Kurtosis']
    variability_columns = [col for col in variability_columns if col in formatted_df.columns]
    print(formatted_df[variability_columns].to_string(index=False))

    print("\nüîç –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–û –£–†–û–í–ù–Ø–ú –ê–ö–¢–ò–í–ù–û–°–¢–ò:")
    distribution_columns = ['Target', '<0.1%', '>5%', '>10%']
    distribution_columns = [col for col in distribution_columns if col in formatted_df.columns]
    print(formatted_df[distribution_columns].to_string(index=False))

    return formatted_df


def analyze_manual_bins(df, feature, target, bins, bin_labels=None):
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Ä—É—á–Ω—ã—Ö –±–∏–Ω–æ–≤ –¥–ª—è fraud detection.

    Parameters:
    -----------
    df : pandas.DataFrame
        –ò—Å—Ö–æ–¥–Ω—ã–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º
    feature : str
        –ù–∞–∑–≤–∞–Ω–∏–µ —á–∏—Å–ª–æ–≤–æ–π –∫–æ–ª–æ–Ω–∫–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä 'login_frequency_7d')
    target : str
        –ö–æ–ª–æ–Ω–∫–∞ —Å —Ç–∞—Ä–≥–µ—Ç–æ–º (0/1)
    bins : list
        –°–ø–∏—Å–æ–∫ –≥—Ä–∞–Ω–∏—Ü –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤, –Ω–∞–ø—Ä–∏–º–µ—Ä: [0, 0.5, 1, 2, 3, 5, 8, 100]
    bin_labels : list, optional
        –ù–∞–∑–≤–∞–Ω–∏—è –±–∏–Ω–æ–≤. –ï—Å–ª–∏ None, –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏

    Returns:
    --------
    pandas.DataFrame
        –¢–∞–±–ª–∏—Ü–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫ –ø–æ –∫–∞–∂–¥–æ–º—É –±–∏–Ω—É —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏
    """

    # –ö–æ–ø–∏—Ä—É–µ–º –¥–∞—Ç–∞—Ñ—Ä–µ–π–º —á—Ç–æ–±—ã –Ω–µ –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª
    df_work = df.copy()

    # –ë–∞–∑–æ–≤—ã–π fraud rate –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ RRN
    baseline_fraud_rate = df_work[target].mean()

    # –ü—Ä–∏—Å–≤–∞–∏–≤–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è–º –±–∏–Ω
    df_work['bin'] = pd.cut(
        df_work[feature],
        bins=bins,
        labels=bin_labels,
        include_lowest=True,
        right=False  # [a, b) - –ø—Ä–∞–≤–∞—è –≥—Ä–∞–Ω–∏—Ü–∞ –Ω–µ –≤–∫–ª—é—á–∞–µ—Ç—Å—è
    )

    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –±–∏–Ω–∞–º
    groups = df_work.groupby('bin', observed=True)

    # –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞
    result = []

    for bin_name, group in groups:

        values = group[feature].values

        # –ó–∞—â–∏—Ç–∞ –æ—Ç –ø—É—Å—Ç—ã—Ö –±–∏–Ω–æ–≤
        if len(values) == 0:
            result.append({
                'bin': bin_name,
                'count': 0,
                'fraud_count': 0,
                'fraud_rate': 0,
                'RRN': np.nan,
                'SE': np.nan,
                'CI_lower': np.nan,
                'CI_upper': np.nan,
                'CI_width': np.nan,
                'reliability': 'NO_DATA',
                'mean': np.nan,
                'median': np.nan,
                'std': np.nan,
                'CV%': np.nan,
                'Q25': np.nan,
                'Q75': np.nan,
                'IQR': np.nan,
                'pct_nonzero': np.nan
            })
            continue

        # === –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ú–ï–¢–†–ò–ö–ò –î–õ–Ø FRAUD DETECTION ===

        count = len(values)
        fraud_count = group[target].sum()
        fraud_rate = fraud_count / count if count > 0 else 0

        # üî¥ –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï 1: RRN = –æ—Ç–Ω–æ—à–µ–Ω–∏–µ fraud rates, –∞ –Ω–µ –∫–≤–∞—Ä—Ç–∏–ª–µ–π!
        # RRN (Relative Risk Ratio) - –≤–æ —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ —Ä–∏—Å–∫ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç baseline
        RRN = fraud_rate / baseline_fraud_rate if baseline_fraud_rate > 0 else np.nan

        # üî¥ –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï 2: SE –¥–ª—è –±–∏–Ω–æ–º–∏–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è (fraud rate)
        # Standard Error –¥–ª—è –ø—Ä–æ–ø–æ—Ä—Ü–∏–∏
        SE_fraud = np.sqrt(fraud_rate * (1 - fraud_rate) / count) if count > 0 else np.nan

        # 95% Confidence Interval –¥–ª—è fraud rate
        CI_lower = max(0, fraud_rate - 1.96 * SE_fraud) if not np.isnan(SE_fraud) else np.nan
        CI_upper = min(1, fraud_rate + 1.96 * SE_fraud) if not np.isnan(SE_fraud) else np.nan
        CI_width = CI_upper - CI_lower if not np.isnan(CI_lower) else np.nan

        # –û—Ü–µ–Ω–∫–∞ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –±–∏–Ω–∞
        if count >= 500 and CI_width < 0.03:
            reliability = 'HIGH'
        elif count >= 200 and CI_width < 0.05:
            reliability = 'MEDIUM'
        elif count >= 50:
            reliability = 'LOW'
        else:
            reliability = 'VERY_LOW'

        # === –î–ï–°–ö–†–ò–ü–¢–ò–í–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–†–ò–ó–ù–ê–ö–ê ===

        mean = np.mean(values)
        median = np.median(values)
        std = np.std(values, ddof=1) if count > 1 else 0

        # SE –¥–ª—è —Å—Ä–µ–¥–Ω–µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–∞ (–Ω–µ fraud rate!)
        SE_mean = std / np.sqrt(count) if count > 1 else 0

        # Coefficient of Variation
        CV = (std / mean * 100) if mean != 0 else 0

        Q25 = np.percentile(values, 25)
        Q75 = np.percentile(values, 75)
        IQR = Q75 - Q25  # Interquartile Range

        # –ü—Ä–æ—Ü–µ–Ω—Ç –Ω–µ–Ω—É–ª–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        pct_nonzero = (values != 0).sum() / count * 100 if count > 0 else 0

        result.append({
            # –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –±–∏–Ω–∞
            'bin': str(bin_name),

            # === FRAUD –ú–ï–¢–†–ò–ö–ò (–ö–†–ò–¢–ò–ß–ù–û!) ===
            'count': count,
            'fraud_count': int(fraud_count),
            'fraud_rate': round(fraud_rate * 100, 3),  # –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö
            'RRN': round(RRN, 3),  # ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–´–ô RRN
            'SE': round(SE_fraud * 100, 3),  # ‚úÖ SE –¥–ª—è fraud rate (–≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö)
            'CI_lower': round(CI_lower * 100, 3),  # –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö
            'CI_upper': round(CI_upper * 100, 3),  # –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö
            'CI_width': round(CI_width * 100, 3),  # –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö
            'reliability': reliability,

            # === –î–ï–°–ö–†–ò–ü–¢–ò–í–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê ===
            'mean': round(mean, 3),
            'median': round(median, 3),
            'std': round(std, 3),
            'SE_mean': round(SE_mean, 3),  # SE –¥–ª—è —Å—Ä–µ–¥–Ω–µ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞
            'CV%': round(CV, 2),
            'Q25': round(Q25, 3),
            'Q75': round(Q75, 3),
            'IQR': round(IQR, 3),
            'pct_nonzero': round(pct_nonzero, 2)
        })

    result_df = pd.DataFrame(result)

    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –ø–æ—Ä—è–¥–∫—É –±–∏–Ω–æ–≤ (–µ—Å–ª–∏ bin - –∫–∞—Ç–µ–≥–æ—Ä–∏—è —Å –ø–æ—Ä—è–¥–∫–æ–º)
    if bin_labels is not None:
        result_df['bin'] = pd.Categorical(result_df['bin'], categories=bin_labels, ordered=True)
        result_df = result_df.sort_values('bin')

    return result_df

def visualize_bin_analysis(analysis_df, feature_name):
    """
    –í—ã–≤–æ–¥–∏—Ç –∫—Ä–∞—Å–∏–≤—É—é –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ –±–∏–Ω–æ–≤
    """
    import matplotlib.pyplot as plt
    import seaborn as sns

    print("=" * 80)
    print(f"üìä –ê–ù–ê–õ–ò–ó –ü–†–ò–ó–ù–ê–ö–ê: {feature_name}")
    print("=" * 80)

    # –û—Å–Ω–æ–≤–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞
    print("\nüéØ FRAUD –ú–ï–¢–†–ò–ö–ò –ü–û –ë–ò–ù–ê–ú:\n")
    display_cols = ['bin', 'count', 'fraud_count', 'fraud_rate', 'RRN',
                    'CI_width', 'reliability']
    print(analysis_df[display_cols].to_string(index=False))

    # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –æ –Ω–µ–Ω–∞–¥–µ–∂–Ω—ã—Ö –±–∏–Ω–∞—Ö
    unreliable = analysis_df[analysis_df['reliability'].isin(['LOW', 'VERY_LOW'])]
    if len(unreliable) > 0:
        print("\n‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –ù–µ–Ω–∞–¥–µ–∂–Ω—ã–µ –±–∏–Ω—ã (—Ç—Ä–µ–±—É—é—Ç –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è):")
        print(unreliable[['bin', 'count', 'fraud_rate', 'CI_width', 'reliability']].to_string(index=False))

    # –í—ã–≤–æ–¥—ã
    print("\n‚úÖ –í–´–í–û–î–´:")

    high_risk = analysis_df[
        (analysis_df['RRN'] >= 1.5) &
        (analysis_df['reliability'].isin(['HIGH', 'MEDIUM']))
        ]
    if len(high_risk) > 0:
        print(f"\nüî¥ –ë–∏–Ω—ã —Å –ü–û–í–´–®–ï–ù–ù–´–ú —Ä–∏—Å–∫–æ–º (RRN ‚â• 1.5, –Ω–∞–¥–µ–∂–Ω—ã–µ):")
        for _, row in high_risk.iterrows():
            print(f"   ‚Ä¢ {row['bin']}: FR={row['fraud_rate']:.2f}%, RRN={row['RRN']:.2f}, n={row['count']}")

    low_risk = analysis_df[
        (analysis_df['RRN'] <= 0.7) &
        (analysis_df['reliability'].isin(['HIGH', 'MEDIUM']))
        ]
    if len(low_risk) > 0:
        print(f"\nüü¢ –ë–∏–Ω—ã —Å –ü–û–ù–ò–ñ–ï–ù–ù–´–ú —Ä–∏—Å–∫–æ–º (RRN ‚â§ 0.7, –Ω–∞–¥–µ–∂–Ω—ã–µ):")
        for _, row in low_risk.iterrows():
            print(f"   ‚Ä¢ {row['bin']}: FR={row['fraud_rate']:.2f}%, RRN={row['RRN']:.2f}, n={row['count']}")

    # –ì—Ä–∞—Ñ–∏–∫
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))

    # 1. Fraud Rate —Å –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–º–∏ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞–º–∏
    ax1 = axes[0, 0]
    reliable = analysis_df[analysis_df['reliability'].isin(['HIGH', 'MEDIUM'])]
    unreliable = analysis_df[~analysis_df['reliability'].isin(['HIGH', 'MEDIUM'])]

    ax1.bar(range(len(reliable)), reliable['fraud_rate'],
            color='steelblue', alpha=0.7, label='–ù–∞–¥–µ–∂–Ω—ã–µ')
    ax1.bar(range(len(reliable), len(analysis_df)), unreliable['fraud_rate'],
            color='lightcoral', alpha=0.5, label='–ù–µ–Ω–∞–¥–µ–∂–Ω—ã–µ')

    # –î–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã
    for i, row in analysis_df.iterrows():
        ax1.errorbar(i, row['fraud_rate'],
                     yerr=row['CI_width'] / 2,
                     fmt='none', color='black', capsize=5, alpha=0.5)

    ax1.set_xlabel('Bin')
    ax1.set_ylabel('Fraud Rate (%)')
    ax1.set_title('Fraud Rate –ø–æ –±–∏–Ω–∞–º —Å 95% CI')
    ax1.set_xticks(range(len(analysis_df)))
    ax1.set_xticklabels(analysis_df['bin'], rotation=45, ha='right')
    ax1.legend()
    ax1.grid(axis='y', alpha=0.3)

    # 2. RRN
    ax2 = axes[0, 1]
    colors = ['green' if r <= 0.7 else 'red' if r >= 1.5 else 'gray'
              for r in analysis_df['RRN']]
    ax2.bar(range(len(analysis_df)), analysis_df['RRN'], color=colors, alpha=0.7)
    ax2.axhline(y=1.0, color='black', linestyle='--', label='Baseline (RRN=1.0)')
    ax2.axhline(y=1.5, color='red', linestyle=':', alpha=0.5, label='High Risk (1.5)')
    ax2.axhline(y=0.7, color='green', linestyle=':', alpha=0.5, label='Low Risk (0.7)')
    ax2.set_xlabel('Bin')
    ax2.set_ylabel('RRN (Relative Risk Ratio)')
    ax2.set_title('–û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∏—Å–∫ –ø–æ –±–∏–Ω–∞–º')
    ax2.set_xticks(range(len(analysis_df)))
    ax2.set_xticklabels(analysis_df['bin'], rotation=45, ha='right')
    ax2.legend()
    ax2.grid(axis='y', alpha=0.3)

    # 3. –†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ –∏ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å
    ax3 = axes[1, 0]
    reliability_colors = {
        'HIGH': 'green',
        'MEDIUM': 'orange',
        'LOW': 'red',
        'VERY_LOW': 'darkred',
        'NO_DATA': 'gray'
    }
    colors = [reliability_colors.get(r, 'gray') for r in analysis_df['reliability']]
    ax3.bar(range(len(analysis_df)), analysis_df['count'], color=colors, alpha=0.7)
    ax3.axhline(y=500, color='green', linestyle='--', alpha=0.5, label='High reliability (500)')
    ax3.axhline(y=200, color='orange', linestyle='--', alpha=0.5, label='Medium reliability (200)')
    ax3.set_xlabel('Bin')
    ax3.set_ylabel('Sample Size')
    ax3.set_title('–†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ –∏ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å')
    ax3.set_xticks(range(len(analysis_df)))
    ax3.set_xticklabels(analysis_df['bin'], rotation=45, ha='right')
    ax3.legend()
    ax3.set_yscale('log')
    ax3.grid(axis='y', alpha=0.3)

    # 4. –®–∏—Ä–∏–Ω–∞ –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞
    ax4 = axes[1, 1]
    ax4.bar(range(len(analysis_df)), analysis_df['CI_width'],
            color='steelblue', alpha=0.7)
    ax4.axhline(y=3, color='green', linestyle='--', alpha=0.5, label='–£–∑–∫–∏–π CI (3%)')
    ax4.axhline(y=5, color='orange', linestyle='--', alpha=0.5, label='–ü—Ä–∏–µ–º–ª–µ–º—ã–π CI (5%)')
    ax4.set_xlabel('Bin')
    ax4.set_ylabel('CI Width (%)')
    ax4.set_title('–®–∏—Ä–∏–Ω–∞ –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞')
    ax4.set_xticks(range(len(analysis_df)))
    ax4.set_xticklabels(analysis_df['bin'], rotation=45, ha='right')
    ax4.legend()
    ax4.grid(axis='y', alpha=0.3)

    plt.tight_layout()
    plt.show()

num = 2

if __name__ == '__main__':
    if num == 0:
        compare_with_join()
    elif num == 1:
        df = load_data()
        logins_0 = df[df['target'] == 0]['logins_last_7_days']
        logins_1 = df[df['target'] == 1]['logins_last_7_days']

        print(f"üìä –î–ê–ù–ù–´–ï –î–õ–Ø –ê–ù–ê–õ–ò–ó–ê:")
        print(f"Target=0: {len(logins_0)} –∑–∞–ø–∏—Å–µ–π")
        print(f"Target=1: {len(logins_1)} –∑–∞–ø–∏—Å–µ–π")
        print(f"–û–±—â–∏–π fraud rate: {len(logins_1) / len(df) * 100:.2f}%")

        automatic_binning_analysis(df)
        trend_data = trend_analysis(df)
        outliers = tail_analysis(df)
        r_squared = nonlinear_analysis(df)
        likelihood_ratio, x_range = kde_analysis(df)

        final_report(df, trend_data, r_squared, likelihood_ratio, x_range)

        # stats_df = calculate_detailed_stats(df, value_column='logins_last_7_days', target_column='target')
        # formatted_stats = print_detailed_stats(stats_df)

    elif num == 2:
        df = load_data()
        bins = [0, 5, 20, 999]
        labels = ["0‚Äì4", "5‚Äì19", "20+"]
        result = analyze_manual_bins(
            df=df,
            feature='logins_last_7_days',
            target='target',
            bins=bins,
            bin_labels=labels
        )

        visualize_bin_analysis(result, 'logins_last_7_days')