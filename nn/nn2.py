import sqlite3
from pathlib import Path

import numpy as np
import pandas as pd
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split, StratifiedKFold
from catboost import CatBoostClassifier, Pool
from sklearn.metrics import (
    roc_auc_score, classification_report, accuracy_score,
    precision_score, recall_score, f1_score, confusion_matrix
)
import warnings
warnings.filterwarnings('ignore')

script_dir = Path(__file__).parent
db_path = script_dir.parent / 'clean_data.db'
conn = sqlite3.connect(db_path)
# conn = sqlite3.connect("../clean_data.db")
query = """
SELECT
    cst_dim_id, 
    transdate,
    transdatetime,
    amount,
    monthly_os_changes,
    monthly_phone_model_changes,
    logins_last_7_days,
    logins_last_30_days,
    login_frequency_7d,
    login_frequency_30d,
    freq_change_7d_vs_mean,
    logins_7d_over_30d_ratio,
    avg_login_interval_30d,
    std_login_interval_30d,
    var_login_interval_30d,
    ewm_login_interval_7d,
    burstiness_login_interval,
    fano_factor_login_interval,
    zscore_avg_login_interval_7d,
    target
FROM unique_transactions
"""
df= pd.read_sql_query(query, conn)
conn.close()

print("=== –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• ===")
print(f"–ö–æ–ª-–≤–æ –∑–∞–ø–∏—Å–µ–π: {len(df)}")
print(f"–ö–æ–ª-–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(df.columns)}")
print("–°—Ç–æ–ª–±—Ü—ã:", list(df.columns))

target_counts = df['target'].value_counts()
target_percentage = df['target'].value_counts(normalize=True) * 100

print("\n=== –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï TARGET ===")
print(f"Class 0 (–ª–µ–≥–∏—Ç–∏–º–Ω—ã–µ): {target_counts[0]} –∑–∞–ø–∏—Å–µ–π ({target_percentage[0]:.2f}%)")
print(f"Class 1 (–º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ): {target_counts[1]} –∑–∞–ø–∏—Å–µ–π ({target_percentage[1]:.2f}%)")
print(f"–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(df)}")


def transform_frequency_column(value):
    """
    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å—Ç—Ä–æ–∫—É, —Å–æ–¥–µ—Ä–∂–∞—â—É—é –¥–∞—Ç—É (–Ω–∞–ø—Ä–∏–º–µ—Ä, '01.–º–∞—Ä'), –≤ —á–∏—Å–ª–æ–≤–æ–π —Ñ–æ—Ä–º–∞—Ç (–Ω–∞–ø—Ä–∏–º–µ—Ä, 1.03).
    –ö–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ —á–∏—Å–ª–æ–≤—ã–µ —Å—Ç—Ä–æ–∫–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, '0.9333...') –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π.
    """
    s = str(value)
    s = s.strip().lower()  # –£–¥–∞–ª—è–µ–º –ø—Ä–æ–±–µ–ª—ã –∏ –ø—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏

    # –ü—Ä–æ–≤–µ—Ä–∫–∞, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —Å—Ç—Ä–æ–∫–∞ –º–µ—Å—è—Ü—ã
    for text_month, num_month in month_map.items():
        if text_month in s:
            try:
                # 01.–º–∞—Ä -> 01.03 (–î–µ–Ω—å.–ú–µ—Å—è—Ü)
                day_part = s.split('.')[0]

                # –°–æ–µ–¥–∏–Ω—è–µ–º '–¥–µ–Ω—å' –∏ '–Ω–æ–º–µ—Ä –º–µ—Å—è—Ü–∞' —á–µ—Ä–µ–∑ —Ç–æ—á–∫—É
                new_value = f"{day_part}.{num_month}"

                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —á–∏—Å–ª–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
                return float(new_value)
            except:
                # –ï—Å–ª–∏ —á—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º NaN
                return np.nan

                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö —á–∏—Å–ª–æ–≤—ã—Ö —Å—Ç—Ä–æ–∫ (—Å –∑–∞–ø—è—Ç–æ–π –∏–ª–∏ —Ç–æ—á–∫–æ–π)
    s = s.replace(',', '.')

    # –ü–æ–ø—ã—Ç–∫–∞ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –æ—á–∏—â–µ–Ω–Ω—É—é —Å—Ç—Ä–æ–∫—É –≤ —á–∏—Å–ª–æ
    try:
        return float(s)
    except ValueError:
        return np.nan

# =========================================================================================================================== #
#               –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–•
# =========================================================================================================================== #
df['transdatetime'] = pd.to_datetime(df['transdatetime'])
df['transdate_day'] = df['transdatetime'].dt.day
df['transdate_dayofweek'] = df['transdatetime'].dt.dayofweek
df['transdate_hour'] = df['transdatetime'].dt.hour
df['transdate_is_business_hours'] = ((df['transdatetime'].dt.hour >= 10) &
                                     (df['transdatetime'].dt.hour <= 18)).astype(int)

df['transdate_year'] = df['transdatetime'].dt.year
df['transdate_month'] = df['transdatetime'].dt.month
df['transdate_minute'] = df['transdatetime'].dt.minute
df['transdate_week'] = df['transdatetime'].dt.isocalendar().week
df['transdate_quarter'] = df['transdatetime'].dt.quarter

df['amount_log'] = np.log1p(df['amount'])

df['total_device_changes'] = df['monthly_os_changes'] + df['monthly_phone_model_changes']

month_map = {
    '—è–Ω–≤': '01', '—Ñ–µ–≤': '02', '–º–∞—Ä': '03', '–∞–ø—Ä': '04',
    '–º–∞–π': '05', '–∏—é–Ω': '06', '–∏—é–ª': '07', '–∞–≤–≥': '08',
    '—Å–µ–Ω': '09', '–æ–∫—Ç': '10', '–Ω–æ—è': '11', '–¥–µ–∫': '12'
}
df['login_frequency_30d'] = df['login_frequency_30d'].apply(transform_frequency_column)

df["freq_change_7d_vs_mean"] = (
    df["freq_change_7d_vs_mean"]
    .astype(str)
    .str.strip()
    .str.replace(',', '.') # –ó–∞–º–µ–Ω—è–µ–º –∑–∞–ø—è—Ç—ã–µ –Ω–∞ —Ç–æ—á–∫–∏
)
df["freq_change_7d_vs_mean"] = pd.to_numeric(df["freq_change_7d_vs_mean"], errors='coerce')

df['avg_30d_is_sentinel'] = (df['avg_login_interval_30d'] == -1).astype(int)
df['avg_login_interval_30d_log'] = np.log1p(df['avg_login_interval_30d'])
df.loc[df['avg_login_interval_30d_log'] < 0, 'avg_login_interval_30d_log'] = 0

df['std_30d_is_sentinel'] = (df['std_login_interval_30d'] == -1).astype(int)
df['std_login_interval_30d_log'] = np.log1p(df['std_login_interval_30d'])
df.loc[df['std_login_interval_30d_log'] < 0, 'std_login_interval_30d_log'] = 0

df['var_login_interval_30d'] = (
    df['var_login_interval_30d']
    .astype(str)
    .str.strip()
    .str.replace(',', '.')
)
df['var_login_interval_30d'] = pd.to_numeric(
    df['var_login_interval_30d'],
    errors='coerce'
)
df['var_30d_is_sentinel'] = (df['var_login_interval_30d'] == -1).astype(int)
df['var_login_interval_30d_log'] = np.log1p(df['var_login_interval_30d'])
df.loc[df['var_login_interval_30d_log'] < 0, 'var_login_interval_30d_log'] = 0

df['ewm_7d_is_sentinel'] = (df['ewm_login_interval_7d'] == -1).astype(int)
df['ewm_login_interval_7d_log'] = np.log1p(df['ewm_login_interval_7d'])
df.loc[df['ewm_login_interval_7d_log'] < 0, 'ewm_login_interval_7d_log'] = 0

df['fano_factor_is_sentinel'] = (df['fano_factor_login_interval'] == -1).astype(int)
df['fano_factor_login_interval_log'] = np.log1p(df['fano_factor_login_interval'])
df.loc[df['fano_factor_login_interval_log'] < 0, 'fano_factor_login_interval_log'] = 0


df['amount_x_hour'] = df['amount_log'] * df['transdate_hour']
df['amount_x_is_business'] = df['amount_log'] * df['transdate_is_business_hours']
df['amount_x_weekend'] = df['amount_log'] * (df['transdate_dayofweek'] >= 5).astype(int)

# –°–µ–∑–æ–Ω–Ω–æ—Å—Ç—å –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
df['zscore_x_hour'] = df['zscore_avg_login_interval_7d'] * df['transdate_hour']
df['zscore_x_day'] = df['zscore_avg_login_interval_7d'] * df['transdate_day']



df['amount_x_hour_x_quarter'] = df['amount_log'] * df['transdate_hour'] * df['transdate_quarter']
df['amount_x_zscore'] = df['amount_log'] * df['zscore_avg_login_interval_7d']
df['activity_volatility'] = df['burstiness_login_interval'] * df['zscore_avg_login_interval_7d']
df['suspicious_behavior_device'] = (
    (df['zscore_avg_login_interval_7d'] > 2) &
    (df['monthly_os_changes'] > 0)
).astype(int)
df['risk_profile'] = (
    df['zscore_avg_login_interval_7d'] * 0.3 +
    df['burstiness_login_interval'] * 0.3 +
    df['fano_factor_login_interval_log'] * 0.2 +
    df['monthly_os_changes'] * 0.2
)
df['amount_x_burstiness'] = df['amount_log'] * df['burstiness_login_interval']
df['month_x_week_x_quarter'] = df['transdate_month'] * df['transdate_week'] * df['transdate_quarter']
# =========================================================================================================================== #
# =========================================================================================================================== #

features_final = [
    # –í—Ä–µ–º–µ–Ω–Ω—ã–µ
    "transdate_day",
    # "transdate_dayofweek",
    "transdate_hour",
    # "transdate_is_business_hours",
    # "transdate_year",
    "transdate_month",
    # "transdate_minute",
    "transdate_week",
    "transdate_quarter",

    # –°—É–º–º–∞ –∏ –ò–∑–º–µ–Ω–µ–Ω–∏—è –£—Å—Ç—Ä–æ–π—Å—Ç–≤
    "amount_log",
    # "total_device_changes",
    "monthly_os_changes",
    "monthly_phone_model_changes",

    # –ò—Å—Ö–æ–¥–Ω—ã–µ –ü–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ (—Ç–æ–∂–µ –º–æ–≥—É—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å NaN –ø–æ—Å–ª–µ to_numeric)
    "logins_last_7_days",
    "logins_last_30_days",
    # "login_frequency_7d",
    # "login_frequency_30d",
    "freq_change_7d_vs_mean",
    # "logins_7d_over_30d_ratio",
    "burstiness_login_interval",
    "zscore_avg_login_interval_7d",

    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–Ω—ã–µ –ò–Ω—Ç–µ—Ä–≤–∞–ª—ã
    "avg_login_interval_30d_log",
    "std_login_interval_30d_log",
    # "var_login_interval_30d_log",
    "ewm_login_interval_7d_log",
    "fano_factor_login_interval_log",

    # –§–ª–∞–≥–∏ –ú–∞—Ä–∫–µ—Ä–æ–≤ (-1)
    # "avg_30d_is_sentinel",
    # "std_30d_is_sentinel",
    # "var_30d_is_sentinel",
    # "ewm_7d_is_sentinel",
    # "fano_factor_is_sentinel",

    'amount_x_hour',
    'amount_x_is_business',
    # 'amount_x_weekend',

    # –ü–†–û–í–ï–†–ò–¢–¨ –ü–û–ù–ò–ñ–ê–Æ–¢ –ò–õ–ò –ù–ï–¢
    'zscore_x_hour',
    # 'zscore_x_day',

    'amount_x_hour_x_quarter',
    # 'amount_x_zscore',
    # 'activity_volatility',
    # 'suspicious_behavior_device',
    # 'risk_profile',
    # 'amount_x_burstiness',
    'month_x_week_x_quarter'
]

CATEGORICAL_FEATURES = [
    # –í—Ä–µ–º–µ–Ω–Ω—ã–µ (—Ü–∏–∫–ª–∏—á–Ω—ã–µ/–¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–µ)
    # "transdate_dayofweek",
    "transdate_hour",
    "transdate_week",
    "transdate_day",
    "transdate_month",
    "transdate_quarter",

    # –°—á–µ—Ç—á–∏–∫–∏ (–¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è 0-N)
    "monthly_os_changes",
    "monthly_phone_model_changes",
    # "total_device_changes"
]

# =========================================================================================================================== #
# =========================================================================================================================== #

X = df[features_final]
y = df['target']

# === 3. –†–ê–ó–î–ï–õ–ï–ù–ò–ï –î–ê–ù–ù–´–• (Train / Validation / Test) ===
# –®–∞–≥ 1: –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –û–±—É—á–µ–Ω–∏–µ+–í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –¢–µ—Å—Ç (80%/20%)
X_train_val, X_test, y_train_val, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# –®–∞–≥ 2: –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –û–±—É—á–µ–Ω–∏–µ –∏ –í–∞–ª–∏–¥–∞—Ü–∏—é (70%/10% –æ—Ç –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞)
# 10% –æ—Ç 80% (X_train_val) = 12.5%
X_train, X_val, y_train, y_val = train_test_split(
    X_train_val, y_train_val, test_size=0.125, random_state=42, stratify=y_train_val
)

print(f"\n–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö: Train: {len(X_train)} | Val: {len(X_val)} | Test: {len(X_test)}")
# =========================================================================================================================== #

# === 4. –ò–ú–ü–¨–Æ–¢–ê–¶–ò–Ø NaN –ò –ü–†–ò–í–ï–î–ï–ù–ò–ï –¢–ò–ü–û–í (Train/Val/Test) ===
print("\n=== –û–ë–†–ê–ë–û–¢–ö–ê NaN –ò –ü–†–ò–í–ï–î–ï–ù–ò–ï –¢–ò–ü–û–í ===")
imputer = SimpleImputer(strategy='median')

# 1. –û–±—É—á–∞–µ–º –∏–º–ø—å—é—Ç–µ—Ä —Ç–æ–ª—å–∫–æ –Ω–∞ TRAIN
X_train_imputed = imputer.fit_transform(X_train)

# 2. –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä—É–µ–º TRAIN, VAL –∏ TEST
X_val_imputed = imputer.transform(X_val)
X_test_imputed = imputer.transform(X_test)

# –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤ DataFrame
X_train_final = pd.DataFrame(X_train_imputed, columns=features_final)
X_val_final = pd.DataFrame(X_val_imputed, columns=features_final)
X_test_final = pd.DataFrame(X_test_imputed, columns=features_final)

# 3. –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö —Ç–∏–ø–æ–≤ –∫ INT
for col in CATEGORICAL_FEATURES:
    for df_final in [X_train_final, X_val_final, X_test_final]:
        df_final[col] = df_final[col].round(0).astype(int)

print("–¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö —É—Å–ø–µ—à–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω—ã.")
# =========================================================================================================================== #

# =========================================================================================================================== #
#               RFECV - –¥–ª—è –≤—ã–±–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–∞
# =========================================================================================================================== #
# import pandas as pd
# from lightgbm import LGBMClassifier
# from sklearn.feature_selection import RFECV
# from sklearn.model_selection import StratifiedKFold # –î–ª—è –Ω–∞–¥–µ–∂–Ω–æ–π CV
# from sklearn.metrics import roc_auc_score
# import numpy as np
#
# # 1. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –í–°–ï–• —Ç–µ–∫—É—â–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
# # –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –∑–¥–µ—Å—å –≤—Å–µ –≤–∞—à–∏ –Ω–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –∏ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏
# features_final = X_train_final.columns.tolist()
#
# # 2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ë–∞–∑–æ–≤–æ–π –ú–æ–¥–µ–ª–∏ (LGBM)
# # LGBM –±—ã—Å—Ç—Ä, —á—Ç–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ –¥–ª—è RFECV, –∫–æ—Ç–æ—Ä—ã–π –º–Ω–æ–≥–æ —Ä–∞–∑ –ø–µ—Ä–µ–æ–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å.
# # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ —É–∂–µ –±–ª–∏–∑–∫–∏ –∫ –æ–ø—Ç–∏–º—É–º—É (–Ω–∏–∑–∫–∏–π LR, —Å—Ä–µ–¥–Ω—è—è –≥–ª—É–±–∏–Ω–∞)
# lgbm_model = LGBMClassifier(
#     n_estimators=500,           # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤ (–∏—Ç–µ—Ä–∞—Ü–∏–π)
#     learning_rate=0.03,
#     max_depth=6,
#     # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ä—É—á–Ω–æ–µ –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ (–¥–ª—è LGBM —ç—Ç–æ weight, –∞ –Ω–µ class_weights)
#     # 5925 / 88 ‚âà 67.3. –û–∫—Ä—É–≥–ª–∏–º –¥–æ 70.
#     scale_pos_weight=70,
#     random_state=42,
#     n_jobs=-1, # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ —è–¥—Ä–∞
#     verbose=-1 # –û—Ç–∫–ª—é—á–∞–µ–º –≤—ã–≤–æ–¥ –≤ —Ü–∏–∫–ª–µ
# )
#
# # 3. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ RFECV
# # –ò—Å–ø–æ–ª—å–∑—É–µ–º StratifiedKFold –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤ –ø—Ä–∏ CV
# # cv=5 –æ–∑–Ω–∞—á–∞–µ—Ç 5-–∫—Ä–∞—Ç–Ω–∞—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
# rfecv = RFECV(
#     estimator=lgbm_model,
#     step=1, # –£–¥–∞–ª—è—Ç—å –ø–æ –æ–¥–Ω–æ–º—É –ø—Ä–∏–∑–Ω–∞–∫—É –Ω–∞ –∫–∞–∂–¥–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏
#     cv=StratifiedKFold(5),
#     scoring='roc_auc',
#     n_jobs=-1
# )
#
# # 4. –ó–∞–ø—É—Å–∫ RFECV –Ω–∞ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ
# # RFECV –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–ø–æ–ª–Ω–∏—Ç –æ–±—É—á–µ–Ω–∏–µ, —É–¥–∞–ª–µ–Ω–∏–µ –∏ CV
# print("ü§ñ –ó–∞–ø—É—Å–∫ RFECV...")
# rfecv.fit(X_train_final[features_final], y_train)
#
# # 5. –ü–æ–ª—É—á–µ–Ω–∏–µ –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –ù–∞–±–æ—Ä–∞
# selected_features = [f for f, selected in zip(features_final, rfecv.support_) if selected]
#
# selected_features = [
#     f for f, selected in zip(features_final, rfecv.support_) if selected
# ]
#
# print(f"--- üéØ –û—Ç–æ–±—Ä–∞–Ω–Ω—ã–µ {len(selected_features)} –ü—Ä–∏–∑–Ω–∞–∫–æ–≤ ---")
# for i, feature in enumerate(selected_features, 1):
#     print(f"{i}. {feature}")
#
# print("\n--- –†–ï–ó–£–õ–¨–¢–ê–¢ RFECV ---")
# print(f"ü§ñ RFECV –æ—Ç–æ–±—Ä–∞–ª {len(selected_features)} –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.")
# print(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Å—Ä–µ–¥–Ω–∏–π AUC (CV): {rfecv.cv_results_['mean_test_score'].max():.4f}")
#
# # 6. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ö—Ä–∏–≤–æ–π RFECV
# # –°—Ç—Ä–æ–∏—Ç—Å—è –∫—Ä–∏–≤–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ AUC –æ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
#
#
# # (–≠—Ç–æ—Ç –≥—Ä–∞—Ñ–∏–∫ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –ø–æ—Å–ª–µ –∫–∞–∫–æ–≥–æ —á–∏—Å–ª–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ AUC –Ω–∞—á–∏–Ω–∞–µ—Ç –ø–∞–¥–∞—Ç—å)
# =========================================================================================================================== #
# =========================================================================================================================== #

from catboost import CatBoostClassifier
from sklearn.metrics import classification_report, roc_auc_score

model_w = CatBoostClassifier(
    iterations=10000,
    learning_rate=0.01,
    depth=7,
    loss_function='Logloss',
    eval_metric='AUC',
    l2_leaf_reg=7,
    random_strength=2,
    bagging_temperature=1.5,


    class_weights=[1, 100],
    # auto_class_weights='Balanced', #'SqrtBalanced' - —Ç–æ–∂–µ –º–æ–∂–Ω–æ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å, –Ω–æ –±–∞–∑–∞ 'Balanced'

    early_stopping_rounds=500,
    verbose=200,
    cat_features=CATEGORICAL_FEATURES,
    random_seed=42,

)

# model_w.fit(X_train_final, y_train)

model_w.fit(
        X_train_final, y_train,
        eval_set=(X_val_final, y_val),
        use_best_model=True,
        verbose=200
    )

# preds = model_w.predict(X_test_final)
# probs = model_w.predict_proba(X_test_final)[:, 1]

preds_val = model_w.predict(X_val_final)
probs_val = model_w.predict_proba(X_val_final)[:, 1]

# –ú–µ—Ç–∫–∏ –í–ê–õ–ò–î–ê–¶–ò–ò (—Å–±—Ä–æ—Å–∏–º –∏–Ω–¥–µ–∫—Å, —á—Ç–æ–±—ã —Å–æ–≤–ø–∞–ª —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º–∏)
y_val_aligned = y_val.reset_index(drop=True)

results = pd.DataFrame({
    'Actual_Target': y_val_aligned, # –ò—Å–ø–æ–ª—å–∑—É–µ–º y_val
    'Prob_Fraud': probs_val
})

# –°–º–æ—Ç—Ä–∏–º –Ω–∞ 5 —Å–ª—É—á–∞–µ–≤ —Å —Å–∞–º–æ–π –≤—ã—Å–æ–∫–æ–π –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞
print("\n=== –¢–û–ü-5 –ü–†–û–ì–ù–û–ó–û–í –ú–û–®–ï–ù–ù–ò–ß–ï–°–¢–í–ê (–Ω–∞ VAL) ===")
print(results.sort_values(by='Prob_Fraud', ascending=False).head(5).to_string(index=False))

# =========================================================================================================================== #
#                   –ò–©–ï–ú –ü–û–†–û–ì –í–•–û–î–ê –î–õ–Ø –ú–û–®–ï–ù–ù–ò–ß–ï–°–ö–û–ô –¢–†–ê–ù–ó–ê–ö–¶–ò–ò
# =========================================================================================================================== #

# –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è –º–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏–∏ F1-score (–æ–±—â–µ–≥–æ –±–∞–ª–∞–Ω—Å–∞)
best_f1 = 0
best_thresh = 0.5

for thresh in np.arange(0.005, 0.3, 0.005):
    current_preds = (probs_val > thresh).astype(int)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º probs_val

    if 1 in current_preds:
        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –í–ê–õ-–í–ï–†–û–Ø–¢–ù–û–°–¢–ò (probs_val) —Å –í–ê–õ-–ú–ï–¢–ö–ê–ú–ò (y_val_aligned)
        current_f1 = f1_score(y_val_aligned, current_preds, average='binary', pos_label=1)

        if current_f1 > best_f1:
            best_f1 = current_f1
            best_thresh = thresh

print(f"\n–ù–∞–∏–ª—É—á—à–∏–π F1-score ({best_f1:.4f}) –¥–æ—Å—Ç–∏–≥–Ω—É—Ç –Ω–∞ VAL –ø—Ä–∏ –ø–æ—Ä–æ–≥–µ: {best_thresh:.3f}")

# preds_new = (probs > best_thresh).astype(int)
#
# # =========================================================================================================================== #
# #                   –¢–µ—Å—Ç —Å–≤–æ–µ–≥–æ –ø–æ—Ä–æ–≥–∞
# # =========================================================================================================================== #
# optimal_threshold = 0.012
# probs = model_w.predict_proba(X_test_final)[:, 1]
# preds_final = (probs >= optimal_threshold).astype(int)
# print("\n=== –†–ï–ó–£–õ–¨–¢–ê–¢–´ –° –°–û–ë–°–¢–í–ï–ù–ù–´–ú –ü–û–†–û–ì–û–ú ===")
# print("AUC:", roc_auc_score(y_test, probs))
# print(classification_report(y_test, preds_final))
# # =========================================================================================================================== #
# # =========================================================================================================================== #
#
# print("\n=== –†–ï–ó–£–õ–¨–¢–ê–¢–´ –° –û–ü–¢–ò–ú–ê–õ–¨–ù–´–ú –ü–û–†–û–ì–û–ú ===")
# print(classification_report(y_test, preds_new))
#
# print("\n=== –†–ï–ó–£–õ–¨–¢–ê–¢–´ –° –°–¢–ê–ù–î–ê–†–¢–ù–´–ú –ü–û–†–û–ì–û–ú ===")
# print("AUC:", roc_auc_score(y_test, probs))
# print(classification_report(y_test, preds))

# =========================================================================================================================== #
#                   –¢–µ—Å—Ç —Å–≤–æ–µ–≥–æ –ø–æ—Ä–æ–≥–∞
# =========================================================================================================================== #


probs_test = model_w.predict_proba(X_test_final)[:, 1] # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –Ω–∞ –¢–ï–°–¢
y_test_aligned = y_test.reset_index(drop=True) # –ú–µ—Ç–∫–∏ –¢–ï–°–¢–ê –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è

# 1. –¢–ï–°–¢ –° –°–û–ë–°–¢–í–ï–ù–ù–´–ú –ü–û–†–û–ì–û–ú (0.1)
optimal_threshold = 0.1
preds_custom = (probs_test >= optimal_threshold).astype(int)
print("\n=== –†–ï–ó–£–õ–¨–¢–ê–¢–´ –° –°–û–ë–°–¢–í–ï–ù–ù–´–ú –ü–û–†–û–ì–û–ú (0.1) ===")
print("AUC:", roc_auc_score(y_test_aligned, probs_test))
print(classification_report(y_test_aligned, preds_custom))
cm_custom = confusion_matrix(y_test_aligned, preds_custom)
print("–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ (–ø–æ—Ä–æ–≥ 0.1):\n", cm_custom)
print(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞: {cm_custom[1, 1]}/{cm_custom[1].sum()}")
# =========================================================================================================================== #
# =========================================================================================================================== #
# 2. –¢–ï–°–¢ –° –û–ü–¢–ò–ú–ê–õ–¨–ù–´–ú –ü–û–†–û–ì–û–ú (–Ω–∞–π–¥–µ–Ω–Ω—ã–º –Ω–∞ VAL)
preds_optimal = (probs_test > best_thresh).astype(int)
print("\n=== –†–ï–ó–£–õ–¨–¢–ê–¢–´ –° –û–ü–¢–ò–ú–ê–õ–¨–ù–´–ú –ü–û–†–û–ì–û–ú (VAL F1-score) ===")
print("AUC:", roc_auc_score(y_test_aligned, probs_test))
print(f"–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥: {best_thresh:.4f}")
print(classification_report(y_test_aligned, preds_optimal))
cm_optimal = confusion_matrix(y_test_aligned, preds_optimal)
print("–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ (–æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥):\n", cm_optimal)
print(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞: {cm_optimal[1, 1]}/{cm_optimal[1].sum()}")
# =========================================================================================================================== #
# =========================================================================================================================== #

all_features = X_train_final.columns.tolist()
feature_importances_array = model_w.get_feature_importance()
importance_df = pd.DataFrame({
    "feature": all_features,
    "importance": feature_importances_array
}).sort_values(by="importance", ascending=False)
print("\nüîù –¢–û–ü-20 –í–ê–ñ–ù–´–• –ü–†–ò–ó–ù–ê–ö–û–í:")
print(importance_df.head(20).to_string(index=False))