import datetime
import json
import sqlite3
from pathlib import Path

import joblib
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from catboost import CatBoostClassifier, Pool
from sklearn.metrics import (
    roc_auc_score, classification_report, confusion_matrix, f1_score
)
from sklearn.impute import SimpleImputer
import warnings

warnings.filterwarnings('ignore')

# === 1. –ó–ê–ì–†–£–ó–ö–ê –ò –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• (–û—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å) ===
# ... (–í–∞—à –∫–æ–¥ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏ —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –≤–∫–ª—é—á–∞—è month_map) ...
# –í —Ü–µ–ª—è—Ö –∫—Ä–∞—Ç–∫–æ—Å—Ç–∏ —è —É–¥–∞–ª—è—é –±–ª–æ–∫ –∫–æ–¥–∞, –∫–æ—Ç–æ—Ä—ã–π —É–∂–µ –±—ã–ª –≤ –ø—Ä–æ–º–ø—Ç–µ,
# –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞—è, —á—Ç–æ –æ–Ω –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –∏ —Å–æ–∑–¥–∞–µ—Ç DataFrame 'df'

# –ò–ó–ú–ï–ù–ï–ù–ù–´–ô –ö–û–î:
# –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –≤–∞—à –∫–æ–¥ –∑–∞–≥—Ä—É–∑–∫–∏ –∏ feature engineering –¥–æ —ç—Ç–æ–≥–æ –º–µ—Å—Ç–∞
# –≤—ã–ø–æ–ª–Ω–∏–ª—Å—è –∏ —Å–æ–∑–¥–∞–ª df —Å–æ –≤—Å–µ–º–∏ –Ω—É–∂–Ω—ã–º–∏ —Å—Ç–æ–ª–±—Ü–∞–º–∏
script_dir = Path(__file__).parent
db_path = script_dir.parent / 'clean_data.db'
conn = sqlite3.connect(db_path)
query = """
SELECT
    cst_dim_id, 
    transdate,
    transdatetime,
    amount,
    monthly_os_changes,
    monthly_phone_model_changes,
    logins_last_7_days,
    logins_last_30_days,
    login_frequency_7d,
    login_frequency_30d,
    freq_change_7d_vs_mean,
    logins_7d_over_30d_ratio,
    avg_login_interval_30d,
    std_login_interval_30d,
    var_login_interval_30d,
    ewm_login_interval_7d,
    burstiness_login_interval,
    fano_factor_login_interval,
    zscore_avg_login_interval_7d,
    target
FROM unique_transactions
"""
df = pd.read_sql_query(query, conn)
conn.close()

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ month_map –∏ transform_frequency_column, —á—Ç–æ–±—ã –≤–∞—à –∫–æ–¥ —Ä–∞–±–æ—Ç–∞–ª
month_map = {
    '—è–Ω–≤': '01', '—Ñ–µ–≤': '02', '–º–∞—Ä': '03', '–∞–ø—Ä': '04',
    '–º–∞–π': '05', '–∏—é–Ω': '06', '–∏—é–ª': '07', '–∞–≤–≥': '08',
    '—Å–µ–Ω': '09', '–æ–∫—Ç': '10', '–Ω–æ—è': '11', '–¥–µ–∫': '12'
}


def transform_frequency_column(value):
    s = str(value).strip().lower().replace(',', '.')
    for text_month, num_month in month_map.items():
        if text_month in s:
            try:
                day_part = s.split('.')[0]
                new_value = f"{day_part}.{num_month}"
                return float(new_value)
            except:
                return np.nan
    try:
        return float(s)
    except ValueError:
        return np.nan


# –í–∞—à –±–ª–æ–∫ feature engineering
df['transdatetime'] = pd.to_datetime(df['transdatetime'])
df['transdate_day'] = df['transdatetime'].dt.day
df['transdate_dayofweek'] = df['transdatetime'].dt.dayofweek
df['transdate_hour'] = df['transdatetime'].dt.hour
df['transdate_is_business_hours'] = ((df['transdatetime'].dt.hour >= 10) & (df['transdatetime'].dt.hour <= 18)).astype(
    int)
df['transdate_year'] = df['transdatetime'].dt.year
df['transdate_month'] = df['transdatetime'].dt.month
df['transdate_minute'] = df['transdatetime'].dt.minute
df['transdate_week'] = df['transdatetime'].dt.isocalendar().week.astype(
    int)  # .dt.isocalendar().week –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç Series of UInt32, –Ω—É–∂–Ω–æ int
df['transdate_quarter'] = df['transdatetime'].dt.quarter

df['amount_log'] = np.log1p(df['amount'])
df['total_device_changes'] = df['monthly_os_changes'] + df['monthly_phone_model_changes']
df['login_frequency_30d'] = df['login_frequency_30d'].apply(transform_frequency_column)
df["freq_change_7d_vs_mean"] = pd.to_numeric(df["freq_change_7d_vs_mean"].astype(str).str.strip().str.replace(',', '.'),
                                             errors='coerce')

df['avg_30d_is_sentinel'] = (df['avg_login_interval_30d'] == -1).astype(int)
df['avg_login_interval_30d_log'] = np.log1p(df['avg_login_interval_30d'])
df.loc[df['avg_login_interval_30d_log'] < 0, 'avg_login_interval_30d_log'] = 0

df['std_30d_is_sentinel'] = (df['std_login_interval_30d'] == -1).astype(int)
df['std_login_interval_30d_log'] = np.log1p(df['std_login_interval_30d'])
df.loc[df['std_login_interval_30d_log'] < 0, 'std_login_interval_30d_log'] = 0

df['var_login_interval_30d'] = pd.to_numeric(df['var_login_interval_30d'].astype(str).str.strip().str.replace(',', '.'),
                                             errors='coerce')
df['var_30d_is_sentinel'] = (df['var_login_interval_30d'] == -1).astype(int)
df['var_login_interval_30d_log'] = np.log1p(df['var_login_interval_30d'])
df.loc[df['var_login_interval_30d_log'] < 0, 'var_login_interval_30d_log'] = 0

df['ewm_7d_is_sentinel'] = (df['ewm_login_interval_7d'] == -1).astype(int)
df['ewm_login_interval_7d_log'] = np.log1p(df['ewm_login_interval_7d'])
df.loc[df['ewm_login_interval_7d_log'] < 0, 'ewm_login_interval_7d_log'] = 0

df['fano_factor_is_sentinel'] = (df['fano_factor_login_interval'] == -1).astype(int)
df['fano_factor_login_interval_log'] = np.log1p(df['fano_factor_login_interval'])
df.loc[df['fano_factor_login_interval_log'] < 0, 'fano_factor_login_interval_log'] = 0

df['amount_x_hour'] = df['amount_log'] * df['transdate_hour']
df['amount_x_is_business'] = df['amount_log'] * df['transdate_is_business_hours']
df['amount_x_weekend'] = df['amount_log'] * (df['transdate_dayofweek'] >= 5).astype(int)
df['zscore_x_hour'] = df['zscore_avg_login_interval_7d'] * df['transdate_hour']
df['zscore_x_day'] = df['zscore_avg_login_interval_7d'] * df['transdate_day']
df['amount_x_hour_x_quarter'] = df['amount_log'] * df['transdate_hour'] * df['transdate_quarter']
df['amount_x_zscore'] = df['amount_log'] * df['zscore_avg_login_interval_7d']
df['activity_volatility'] = df['burstiness_login_interval'] * df['zscore_avg_login_interval_7d']
df['suspicious_behavior_device'] = ((df['zscore_avg_login_interval_7d'] > 2) & (df['monthly_os_changes'] > 0)).astype(
    int)
df['risk_profile'] = (df['zscore_avg_login_interval_7d'] * 0.3 + df['burstiness_login_interval'] * 0.3 + df[
    'fano_factor_login_interval_log'] * 0.2 + df['monthly_os_changes'] * 0.2)
df['amount_x_burstiness'] = df['amount_log'] * df['burstiness_login_interval']
df['month_x_week_x_quarter'] = df['transdate_month'] * df['transdate_week'] * df['transdate_quarter']
# =========================================================================================================================== #

# === 2. –§–ò–ù–ê–õ–¨–ù–´–ô –ù–ê–ë–û–† –ü–†–ò–ó–ù–ê–ö–û–í (–û—á–∏—â–µ–Ω–Ω—ã–π –∏ —Å—Ç–∞–±–∏–ª—å–Ω—ã–π) ===
# –ú—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–ø–∏—Å–æ–∫ –∏–∑ 20 —Å—Ç–∞–±–∏–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã –≤—ã–±—Ä–∞–ª–∏ —Ä–∞–Ω–µ–µ.

features_final = [
    # –í—Ä–µ–º–µ–Ω–Ω—ã–µ
    "transdate_day",
    # "transdate_dayofweek",
    "transdate_hour",
    # "transdate_is_business_hours",
    # "transdate_year",
    "transdate_month",
    # "transdate_minute",
    "transdate_week",
    "transdate_quarter",

    # –°—É–º–º–∞ –∏ –ò–∑–º–µ–Ω–µ–Ω–∏—è –£—Å—Ç—Ä–æ–π—Å—Ç–≤
    "amount_log",
    # "total_device_changes",
    "monthly_os_changes",
    "monthly_phone_model_changes",

    # –ò—Å—Ö–æ–¥–Ω—ã–µ –ü–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ (—Ç–æ–∂–µ –º–æ–≥—É—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å NaN –ø–æ—Å–ª–µ to_numeric)
    "logins_last_7_days",
    "logins_last_30_days",
    # "login_frequency_7d",
    # "login_frequency_30d",
    "freq_change_7d_vs_mean",
    # "logins_7d_over_30d_ratio",
    "burstiness_login_interval",
    "zscore_avg_login_interval_7d",

    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–Ω—ã–µ –ò–Ω—Ç–µ—Ä–≤–∞–ª—ã
    "avg_login_interval_30d_log",
    "std_login_interval_30d_log",
    # "var_login_interval_30d_log",
    "ewm_login_interval_7d_log",
    "fano_factor_login_interval_log",

    # –§–ª–∞–≥–∏ –ú–∞—Ä–∫–µ—Ä–æ–≤ (-1)
    # "avg_30d_is_sentinel",
    # "std_30d_is_sentinel",
    # "var_30d_is_sentinel",
    # "ewm_7d_is_sentinel",
    # "fano_factor_is_sentinel",

    'amount_x_hour',
    'amount_x_is_business',
    # 'amount_x_weekend',

    # –ü–†–û–í–ï–†–ò–¢–¨ –ü–û–ù–ò–ñ–ê–Æ–¢ –ò–õ–ò –ù–ï–¢
    'zscore_x_hour',
    # 'zscore_x_day',

    'amount_x_hour_x_quarter',
    # 'amount_x_zscore',
    # 'activity_volatility',
    # 'suspicious_behavior_device',
    # 'risk_profile',
    # 'amount_x_burstiness',
    'month_x_week_x_quarter'
]

CATEGORICAL_FEATURES = [
    # –í—Ä–µ–º–µ–Ω–Ω—ã–µ (—Ü–∏–∫–ª–∏—á–Ω—ã–µ/–¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–µ)
    # "transdate_dayofweek",
    "transdate_hour",
    "transdate_week",
    "transdate_day",
    "transdate_month",
    "transdate_quarter",

    # –°—á–µ—Ç—á–∏–∫–∏ (–¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è 0-N)
    "monthly_os_changes",
    "monthly_phone_model_changes",
    # "total_device_changes"
]

X = df[features_final]
y = df['target']

# === 3. –†–ê–ó–î–ï–õ–ï–ù–ò–ï –î–ê–ù–ù–´–• (Train / Validation / Test) ===
# –®–∞–≥ 1: –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –û–±—É—á–µ–Ω–∏–µ+–í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –¢–µ—Å—Ç (80%/20%)
X_train_val, X_test, y_train_val, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# –®–∞–≥ 2: –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –û–±—É—á–µ–Ω–∏–µ –∏ –í–∞–ª–∏–¥–∞—Ü–∏—é (70%/10% –æ—Ç –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞)
# 10% –æ—Ç 80% (X_train_val) = 12.5%
X_train, X_val, y_train, y_val = train_test_split(
    X_train_val, y_train_val, test_size=0.125, random_state=42, stratify=y_train_val
)

print(f"\n–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö: Train: {len(X_train)} | Val: {len(X_val)} | Test: {len(X_test)}")
# =========================================================================================================================== #

# === 4. –ò–ú–ü–¨–Æ–¢–ê–¶–ò–Ø NaN –ò –ü–†–ò–í–ï–î–ï–ù–ò–ï –¢–ò–ü–û–í (Train/Val/Test) ===
print("\n=== –û–ë–†–ê–ë–û–¢–ö–ê NaN –ò –ü–†–ò–í–ï–î–ï–ù–ò–ï –¢–ò–ü–û–í ===")
imputer = SimpleImputer(strategy='median')

# 1. –û–±—É—á–∞–µ–º –∏–º–ø—å—é—Ç–µ—Ä —Ç–æ–ª—å–∫–æ –Ω–∞ TRAIN
X_train_imputed = imputer.fit_transform(X_train)

# 2. –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä—É–µ–º TRAIN, VAL –∏ TEST
X_val_imputed = imputer.transform(X_val)
X_test_imputed = imputer.transform(X_test)

# –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤ DataFrame
X_train_final = pd.DataFrame(X_train_imputed, columns=features_final)
X_val_final = pd.DataFrame(X_val_imputed, columns=features_final)
X_test_final = pd.DataFrame(X_test_imputed, columns=features_final)

# 3. –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö —Ç–∏–ø–æ–≤ –∫ INT
for col in CATEGORICAL_FEATURES:
    for df_final in [X_train_final, X_val_final, X_test_final]:
        df_final[col] = df_final[col].round(0).astype(int)

print("–¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö —É—Å–ø–µ—à–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω—ã.")
# =========================================================================================================================== #
# –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π
MODELS_DIR = Path("saved_models")
MODELS_DIR.mkdir(exist_ok=True)

model_performances = []

# === 5. –û–ë–£–ß–ï–ù–ò–ï –ê–ù–°–ê–ú–ë–õ–Ø –° –°–û–•–†–ê–ù–ï–ù–ò–ï–ú ===
seeds = [42, 101, 202, 303, 404]
models = []
model_performances = []

print(f"\n=== –ó–ê–ü–£–°–ö –û–ë–£–ß–ï–ù–ò–Ø –ê–ù–°–ê–ú–ë–õ–Ø ({len(seeds)} –º–æ–¥–µ–ª–µ–π) ===")

best_model = None
best_auc = 0
best_model_index = 0

for i, seed in enumerate(seeds):
    print(f"\n[–ú–æ–¥–µ–ª—å {i + 1}/5] –û–±—É—á–µ–Ω–∏–µ —Å random_seed={seed}")

    model = CatBoostClassifier(
        iterations=5000,
        learning_rate=0.01,
        depth=7,
        loss_function='Logloss',
        eval_metric='AUC',
        l2_leaf_reg=5,
        random_strength=1.5,
        bagging_temperature=1.0,
        auto_class_weights='Balanced',
        early_stopping_rounds=500,
        verbose=200,
        cat_features=CATEGORICAL_FEATURES,
        random_seed=seed,
        thread_count=-1
    )

    model.fit(
        X_train_final, y_train,
        eval_set=(X_val_final, y_val),
        use_best_model=True,
        verbose=200
    )

    # –û—Ü–µ–Ω–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ validation set
    val_probs = model.predict_proba(X_val_final)[:, 1]
    val_auc = roc_auc_score(y_val, val_probs)

    models.append(model)
    model_performances.append({
        'model_index': i,
        'seed': seed,
        'val_auc': val_auc,
        'best_iteration': model.get_best_iteration()
    })

    print(f"‚úÖ –ú–æ–¥–µ–ª—å {i + 1} –æ–±—É—á–µ–Ω–∞. Val AUC: {val_auc:.4f}, –õ—É—á—à–∞—è –∏—Ç–µ—Ä–∞—Ü–∏—è: {model.get_best_iteration()}")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –æ—Ç–¥–µ–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
    if val_auc > best_auc:
        best_auc = val_auc
        best_model = model
        best_model_index = i

# === –°–û–•–†–ê–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô ===
print(f"\nüíæ –°–û–•–†–ê–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô...")

# 1. –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –æ—Ç–¥–µ–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
best_model_path = MODELS_DIR / "best_single_model.cbm"
best_model.save_model(str(best_model_path))
print(f"‚úÖ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {best_model_path} (AUC: {best_auc:.4f})")

# 2. –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ—Å—å –∞–Ω—Å–∞–º–±–ª—å
ensemble_path = MODELS_DIR / "ensemble_models"
ensemble_path.mkdir(exist_ok=True)

for i, model in enumerate(models):
    model_path = ensemble_path / f"model_{i}.cbm"
    model.save_model(str(model_path))

print(f"‚úÖ –ê–Ω—Å–∞–º–±–ª—å –∏–∑ {len(models)} –º–æ–¥–µ–ª–µ–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {ensemble_path}")

# 3. –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∞–Ω—Å–∞–º–±–ª—è
metadata = {
    'created_date': datetime.datetime.now().isoformat(),
    'models_count': len(models),
    'best_model_index': best_model_index,
    'best_model_auc': best_auc,
    'model_performances': model_performances,
    'features_used': features_final,
    'categorical_features': CATEGORICAL_FEATURES,
    'test_auc': None,  # –∑–∞–ø–æ–ª–Ω–∏—Ç—Å—è –ø–æ—Å–ª–µ –æ—Ü–µ–Ω–∫–∏
    'optimal_threshold': None
}

metadata_path = MODELS_DIR / "ensemble_metadata.json"
with open(metadata_path, 'w', encoding='utf-8') as f:
    json.dump(metadata, f, indent=2, ensure_ascii=False)

print(f"‚úÖ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {metadata_path}")

# 4. –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–º–ø—É—Ç–µ—Ä –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö
preprocessor_path = MODELS_DIR / "preprocessor.joblib"
joblib.dump({
    'imputer': imputer,
    'feature_names': features_final,
    'categorical_features': CATEGORICAL_FEATURES
}, preprocessor_path)

print(f"‚úÖ –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {preprocessor_path}")

# === 6. –ê–ù–°–ê–ú–ë–õ–ò–†–û–í–ê–ù–ò–ï –ò –§–ò–ù–ê–õ–¨–ù–´–ô –ü–†–û–ì–ù–û–ó ===
print(f"\nüéØ –ê–ù–°–ê–ú–ë–õ–ò–†–û–í–ê–ù–ò–ï...")

# –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –º–æ–¥–µ–ª–µ–π
model_weights = [perf['val_auc'] for perf in model_performances]
model_weights = np.array(model_weights) / sum(model_weights)

print("–í–µ—Å–∞ –º–æ–¥–µ–ª–µ–π –≤ –∞–Ω—Å–∞–º–±–ª–µ:")
for i, (perf, weight) in enumerate(zip(model_performances, model_weights)):
    print(f"  –ú–æ–¥–µ–ª—å {i + 1}: AUC={perf['val_auc']:.4f}, –≤–µ—Å={weight:.3f}")

# –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
final_probs = np.zeros(len(X_test_final))
for i, (model, weight) in enumerate(zip(models, model_weights)):
    final_probs += model.predict_proba(X_test_final)[:, 1] * weight

# === 7. –û–¶–ï–ù–ö–ê –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ===
test_auc = roc_auc_score(y_test, final_probs)
print(f"\nüìä –§–ò–ù–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ê–ù–°–ê–ú–ë–õ–Ø:")
print(f"AUC –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ: {test_auc:.4f}")

# –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
metadata['test_auc'] = test_auc

# –ü–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞
best_f1 = 0
best_thresh = 0.5
for thresh in np.arange(0.005, 0.3, 0.005):
    current_preds = (final_probs > thresh).astype(int)
    if 1 in current_preds:
        current_f1 = f1_score(y_test, current_preds, average='binary', pos_label=1)
        if current_f1 > best_f1:
            best_f1 = current_f1
            best_thresh = thresh

metadata['optimal_threshold'] = best_thresh

print(f"üéØ –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥: {best_thresh:.4f} (F1-score: {best_f1:.4f})")

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
with open(metadata_path, 'w', encoding='utf-8') as f:
    json.dump(metadata, f, indent=2, ensure_ascii=False)

# === –í–´–í–û–î –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ===
print(f"\n{'=' * 50}")
print("üéâ –ê–ù–°–ê–ú–ë–õ–¨ –£–°–ü–ï–®–ù–û –û–ë–£–ß–ï–ù –ò –°–û–•–†–ê–ù–ï–ù!")
print(f"{'=' * 50}")

results_summary = {
    '–õ—É—á—à–∞—è –º–æ–¥–µ–ª—å': f"AUC: {best_auc:.4f}",
    '–ê–Ω—Å–∞–º–±–ª—å': f"AUC: {test_auc:.4f}",
    '–£–ª—É—á—à–µ–Ω–∏–µ': f"+{(test_auc - best_auc) * 100:.2f}%",
    '–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥': f"{best_thresh:.4f}",
    '–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –º–æ–¥–µ–ª–µ–π': f"{len(models)}",
    '–ü–∞–ø–∫–∞ —Å –º–æ–¥–µ–ª—è–º–∏': str(MODELS_DIR)
}

for key, value in results_summary.items():
    print(f"{key:20}: {value}")

# === 8. –ó–ê–ì–†–£–ó–ö–ê –ò –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï –°–û–•–†–ê–ù–ï–ù–ù–´–• –ú–û–î–ï–õ–ï–ô ===
print(f"\nüîß –ü–†–ò–ú–ï–† –ó–ê–ì–†–£–ó–ö–ò –°–û–•–†–ê–ù–ï–ù–ù–´–• –ú–û–î–ï–õ–ï–ô:")

# –ü—Ä–∏–º–µ—Ä –∑–∞–≥—Ä—É–∑–∫–∏ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
loaded_best_model = CatBoostClassifier()
loaded_best_model.load_model(str(best_model_path))
print(f"‚úÖ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π")


# –ü—Ä–∏–º–µ—Ä –∑–∞–≥—Ä—É–∑–∫–∏ –≤—Å–µ–≥–æ –∞–Ω—Å–∞–º–±–ª—è
def load_ensemble(ensemble_dir):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ–≥–æ –∞–Ω—Å–∞–º–±–ª—è"""
    ensemble_dir = Path(ensemble_dir)
    models = []

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    with open(ensemble_dir.parent / "ensemble_metadata.json", 'r', encoding='utf-8') as f:
        metadata = json.load(f)

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –º–æ–¥–µ–ª–∏
    for i in range(metadata['models_count']):
        model = CatBoostClassifier()
        model_path = ensemble_dir / f"model_{i}.cbm"
        model.load_model(str(model_path))
        models.append(model)

    return models, metadata


# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
preprocessor = joblib.load(preprocessor_path)
print(f"‚úÖ –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω")

print(f"\nüí° –î–õ–Ø –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô –ù–ê –ù–û–í–´–• –î–ê–ù–ù–´–•:")
print("1. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å: model = CatBoostClassifier()")
print("2. model.load_model('saved_models/best_single_model.cbm')")
print("3. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ model.predict_proba(new_data)[:, 1]")